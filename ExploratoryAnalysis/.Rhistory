25000*5e6
library("gridExtra") #for saving png files in a specific order into pdf
library("ggplot2")
library('ggdendro')
# library("TSclust")
library("ggmap") #used to plot maps
library("maps")
library("scales") # for function alpha()
library("compiler")  # to speed up the computations!
library("plyr")
library("hexbin") #for hexoganal binning
library("rgeos") #for creating maps
library("png") #for reading png files
library("grid") #for arranging png files
library("data.table") #for faster creation of crosstables from data set & for faster searches of datatables; brings about a lot of speed-up! https://github.com/Rdatatable/data.table/wiki/Getting-started
library("bit64") #for loading data with fread
library("lubridate") #for handling time and date information; http://stackoverflow.com/questions/10705328/extract-hours-and-seconds-from-posixct-for-plotting-purposes-in-r
#install_github("rundel/timezone") #needs terminal commands: http://stackoverflow.com/questions/33381421/how-to-upgrade-proj4-for-rgdal
#sudo apt-get install libgdal-dev libproj-dev
library("timezone") #for getting timezones from lat/long data
root_path <- "~/Dropbox/UZH_Master/Masterarbeit/TwitterEpi/ExploratoryAnalysis" # defining root_path containing all relevant documents
# # LOADING and MERGING DATA FRAMES  ------
#
#   setwd(root_path) # setting WD
#   #function to make  selection of datatable based on pre_set coordinates
#
#   #loading files from sick patients
#   #see http://stackoverflow.com/questions/11433432/importing-multiple-csv-files-into-r for explanation about reading several csv-files at once
#   setwd("C:/Users/DrosoNeuro/Dropbox/UZH_Master/Masterarbeit/TwitterData/tweets_from_todd/csv_files/sick_csv") # temporarily set WD to folder with files from healthy Twitter users
#
#   temp = list.files(pattern="*.csv") #read names of all .csv files
#
#   #creates names from csv-files in folder;
#   names <- setNames(temp, make.names(gsub("*.csv$", "", temp))) #gsub uses regex to replace the specified patterns within a name
#
#   #loading df into environment
#   list2env(lapply(names,fread, header=FALSE), envir = .GlobalEnv) #"fread" reads in the data from csv
#
#   #create a list of all the datatables
#   sick_list <- lapply(attr(names,"names"),get)
#
#   #combine into a single datatable
#   sick_df <- do.call("rbind",sick_list)
#
#   remove(list = attr(names,"names"))#removing single df to save RAM
#   remove(sick_list)#removing sick_list to save RAM
#
#   col_names <- c('userID','longitude','latitude','time','sick','state')
#   colnames(sick_df) <- col_names
#   setkeyv(sick_df,col_names)
#   alarm()
#
#   #loading data from healthy Twitter users
#   setwd("C:/Users/DrosoNeuro/Dropbox/UZH_Master/Masterarbeit/TwitterData/tweets_from_todd/csv_files/one_hundred_csv") # temporarily set WD to folder with files from healthy Twitter users
#   temp = list.files(pattern="*.csv") #read names of all .csv files
#
#   #creates names from csv-files in folder;
#   names <- setNames(temp, make.names(gsub("*.csv$", "", temp))) #gsub uses regex to replace the specified patterns within a name
#
#   #loading df into environment
#   list2env(lapply(names,fread, header=FALSE), envir = .GlobalEnv)
#
#   #create a list of all the datatables
#   healthy_list <- lapply(attr(names,"names"),get)
#
#   #combine into a single datatable
#   healthy_df <- do.call("rbind",healthy_list)
#
#   remove(list = attr(names,"names"))#removing single df to save RAM
#   remove(healthy_list)#removing sick_list to save RAM
#   remove(list= c("names","temp"))
#
#   colnames(healthy_df) <- col_names
#   setkeyv(healthy_df, col_names) #sets key to column "userID"
#   remove(col_names)
#   alarm()
#
#   setwd(root_path) # set WD back
#
#   save.image(file="Twitter_datatables.RData") #saving loaded datatable to prevent loading it from the excel-files the next time
# EXPLORATORY DATA ANALYSIS ------
setwd(root_path) # set WD back
load(file="Twitter_datatables.RData") #if the code above has been executed once, you can uncomment it and start directly from here
title_plot<-"All tweets from " #generic title plot used in some functions
# to_analyse <- "healthy_df"
# rm(list=setdiff(ls(), to_analyse)) #removes all entries from workspace except for the datatable that shall be analysed
#
#funtion to make selection of datatable based on coordinate (lon_west,lon_est,lat_south,lat_north)
coord_selection  <- function(datatable,coord_selec)
{
selec <- datatable[datatable[,longitude >=coord_selec[1] & longitude <= coord_selec[2] & latitude >= coord_selec[3] & latitude <=coord_selec[4]],]
#selec <- datatable[which(datatable[,"longitude",]>=coord_selec[1] & datatable[,"longitude"] <= coord_selec[2] & datatable[,"latitude"] >= coord_selec[3] & datatable[,"latitude"] <= coord_selec[4]),] #old way to do it with dataframes
}
#function to make selection of datatable based on coordinate (lon_west,lon_est,lat_south,lat_north); also returns index
coord_selection2  <- function(datatable,coord_selec) #
{
selec <- datatable[datatable[,longitude >=coord_selec[1] & longitude <= coord_selec[2] & latitude >= coord_selec[3] & latitude <=coord_selec[4]],]
index <-datatable[,longitude >=coord_selec[1] & longitude <= coord_selec[2] & latitude >= coord_selec[3] & latitude <=coord_selec[4]]
return(list(selec,index))
}
coord_USA <- c(-125,-66,25,50) #select only tweets from mainland USA
sick_df <- coord_selection(sick_df, coord_USA)
healthy_df <- coord_selection(healthy_df,coord_USA)
explore_data <- function(datatable,sickness_state){ #"sickness_state" takes values "sick" or "healthy" and signifies the state that the users represented in the dataste *should* be in
all_users<-unique(datatable[,userID]) #unique returns a vector, data frame or array like x but with duplicate elements/rows removed; in this case = unique return of user_ID
num_users <- length(all_users)
sick_position <- which(datatable[,sick]==1) #gets position of tweets labelled as asick
num_sick_tweets<-sum(datatable[,sick]==1) #returns number of tweets that are labelled as "sick"
#sick_tweets<-datatable[sick_position,5] #returns entries that are labelled as sick
# NB! n sick tweets != n sick users!!!
sick_users<-unique(datatable[sick_position,userID])
num_sick_users <- length(sick_users)
#getting healthy
healthy_position <- which(datatable[,sick]==0)
#healthy_tweets <- unique(datatable[healthy_position,5]) #returns entries that are labelled as healthy
num_healthy_tweets <- sum(datatable[,sick]==0)
healthy_users <- unique(datatable[healthy_position,userID]) #getting healthy users
num_healthy_users <- length(healthy_users)
#check the total number of false labels
if (sickness_state == "sick"){ #checking whether there are any users in a "sick" datatable that have never been sick, i.e. that healthy_users that don't show up in sick_users
false_label <- healthy_users[!(healthy_users %in% sick_users)]
num_false_label <- length(false_label)
}
else if (sickness_state == "healthy"){
false_label <- sick_users
num_false_label <- num_sick_users
}
out <- list(all_users,num_users,sick_position,num_sick_tweets,sick_users,num_sick_users,healthy_position,num_healthy_tweets,healthy_users,num_healthy_users,false_label,num_false_label)
names(out) <- c("all_users","num_users", "sick_position","num_sick_tweets","sick_users","num_sick_users","healthy_position", "num_healthy_tweets", "healthy_users","num_healthy_users","false_label","num_false_label")
return(out)
}
#get preliminary info from datatables
explore_sick <- explore_data(sick_df,"sick")
str(explore_sick)
explore_sick <- list(explore_sick$false_label) #prune list to save memory
names(explore_sick) <- "false_label"
explore_healthy <- explore_data(healthy_df,"healthy")
str(explore_healthy)
explore_healthy <- list(explore_healthy$false_label) #reduce size of list to save memory
names(explore_healthy) <- "false_label"
datatable <- sick_df
tag <- "sick_df"
explore <- explore_sick
setkey(datatable,"userID")
datatable <- datatable[time!=0,] #removing all entries which don't have a system time
datatable[,time:=as.POSIXct(datatable[,time],origin="1970-01-01")] #transforming time from system time to calendar time
datatable[,date:=as.Date(time,format="%Y-%m-%d")] #stripping exact time information
dates <- datatable[,.N,by=.(date)] #".N" is a shortcut for length(current_object), in this case, it outputs the nunber of occurences of each time-stamp in the column "time"; .() is a shorthand for "list"
dates
datatable
to_export <- copy(datable)
to_export <- copy(datatable)
to_export <- to_export[,.(userID,longitude,latitude,time)]
to_export
write.csv(to_export,file="to_export.csv",sep=";")
install.packages("feather")
library("feather")
mtcars
mtcars.
help(write_feather)
getwd()
root_path <- "~/Dropbox/UZH_Master/Masterarbeit/TwitterEpi/TestingCode" # defining root_path containing all test codes
setwd(root_path)
write_feather(mtcars,"mtcars.feather")
mtcars2 <- read_feather("mtcars.feather")
str(mtcars2)
str(mtcars)
library("feather") #for fast exporting and importing of data: http://blog.revolutionanalytics.com/2016/05/feather-package.html
write_feather(to_export,"to_export.feather")
to_import <- read_feather("to_export.feather")
write_feather(mtcars,"mtcars.feather")
mtcars2 <- read_feather("mtcars.feather")
help(fwrite)
help(data.table::fwrite)
help("data.table::fwrite")
??fwrite
help(fread)
help(fwrite)
remove.packages("data.table")
install.packages("data.table",type="source",repos="http://Rdatatable.github.io/data.table")
install.packages("data.table",type="source",repos="http://Rdatatable.github.io/data.table")
library("data.table")
help(fwrite)
rm(list=ls())
root_path <- "~/Dropbox/UZH_Master/Masterarbeit/TwitterEpi/ExploratoryAnalysis" # defining root_path containing all relevant documents
setwd(root_path) # set WD back
load(file="Twitter_datatables.RData") #if the code above has been executed once, you can uncomment it and start directly from here
getwd()
fwrite(healthy_df,"healthy_df.csv") #fwrite needs developmental package of "data.table" for now (as of 2016.09.16)
fwrite(sick_df,"sick_df.csv")
View(sick_df)
rm(list=ls())
sick_df <- fread("sick_df.csv")
head(sick_df)
"Twitter_datatables.RData") #if the code above has been executed once, you can uncomment it and start directly from here
load(file="Twitter_datatables.RData") #if the code above has been executed once, you can uncomment it and start directly from here
head(sick_df)
head(healthy_df)
remove(list=ls())
load(file="Twitter_datatables.RData") #if the code above has been executed once, you can uncomment it and start directly from here
head(healthy_df)
library("profvis")
profvis({
library("gridExtra") #for saving png files in a specific order into pdf
library("ggplot2")
library('ggdendro')
# library("TSclust")
library("ggmap") #used to plot maps
library("maps")
library("scales") # for function alpha()
library("compiler")  # to speed up the computations!
library("plyr")
library("hexbin") #for hexoganal binning
library("rgeos") #for creating maps
library("png") #for reading png files
library("grid") #for arranging png files
library("data.table") #for faster creation of crosstables from data set & for faster searches of datatables; brings about a lot of speed-up! https://github.com/Rdatatable/data.table/wiki/Getting-started
library("bit64") #for loading data with fread
library("lubridate") #for handling time and date information; http://stackoverflow.com/questions/10705328/extract-hours-and-seconds-from-posixct-for-plotting-purposes-in-r
#install_github("rundel/timezone") #needs terminal commands: http://stackoverflow.com/questions/33381421/how-to-upgrade-proj4-for-rgdal
#sudo apt-get install libgdal-dev libproj-dev
library("timezone") #for getting timezones from lat/long data
library("feather") #for fast exporting and importing of data: http://blog.revolutionanalytics.com/2016/05/feather-package.html
#other possibility for fast exporting data is fwrite() using the data.table package: http://blog.h2o.ai/2016/04/fast-csv-writing-for-r/
)
library("gridExtra") #for saving png files in a specific order into pdf
library("ggplot2")
library('ggdendro')
# library("TSclust")
library("ggmap") #used to plot maps
library("maps")
library("scales") # for function alpha()
library("compiler")  # to speed up the computations!
library("plyr")
library("hexbin") #for hexoganal binning
library("rgeos") #for creating maps
library("png") #for reading png files
library("grid") #for arranging png files
library("data.table") #for faster creation of crosstables from data set & for faster searches of datatables; brings about a lot of speed-up! https://github.com/Rdatatable/data.table/wiki/Getting-started
library("bit64") #for loading data with fread
library("lubridate") #for handling time and date information; http://stackoverflow.com/questions/10705328/extract-hours-and-seconds-from-posixct-for-plotting-purposes-in-r
#install_github("rundel/timezone") #needs terminal commands: http://stackoverflow.com/questions/33381421/how-to-upgrade-proj4-for-rgdal
#sudo apt-get install libgdal-dev libproj-dev
library("timezone") #for getting timezones from lat/long data
library("feather") #for fast exporting and importing of data: http://blog.revolutionanalytics.com/2016/05/feather-package.html
#other possibility for fast exporting data is fwrite() using the data.table package: http://blog.h2o.ai/2016/04/fast-csv-writing-for-r/
root_path <- "~/Dropbox/UZH_Master/Masterarbeit/TwitterEpi/ExploratoryAnalysis" # defining root_path containing all relevant documents
head(sick_df)
sick_df <- fread("sick_df.csv")
sick_df <- fread("sick_df.csv")
head(sick_df)
load(file="Twitter_datatables.RData") #if the code above has been executed once, you can uncomment it and start directly from here
fwrite(healthy_df,"healthy_df.csv") #fwrite needs developmental package of "data.table" for now (as of 2016.09.16)
fwrite(sick_df,"sick_df.csv") #doesn't work yet!!!
sick_df <- fread("sick_df.csv")
head(sick_df)
sick_df2 <- fread("sick_df.csv")
healthy_df2 <- fread("healthy_df.csv")
load(file="Twitter_datatables.RData") #if the code above has been executed once, you can uncomment it and start directly from here
help(write_feather)
write_feather(sick_df, "sick_df.feather")
write_feather(healthy_df,"healthy_df.feather")
help(read_feather)
sick_df2 <- read_feather("sick_df.feather")
healthy_df2 <- read_feather("healthy_df.feather")
head(sick_df)
head(sick_df2)
str(sick_df2)
sick_df2 <- data.table(sick_df2)
sick_df2
sick_df[1,]
sick_df2
sick_df2[1,]
str(sick_df)
new <- copy(sick_df)
new <- new[,userID.=as.integer(userID)]
new <- new[,userID:=as.integer(userID)]
new
head(new)
new[userID==NA,]
new[userID==322,]
new[userID=="NA",]
help("is.na")
is.na(new[userID])
is.na(new[,userID])
sum(is.na(new[,userID]))
sum(is.nan(new[,userID]))
sum(is.na(sick_df[,userID]))
sum(is.na(sick_df2[,userID]))
help(integer64)
new <- copy(sick_df)
new <- new[,userID:=as.integer64.integer(userID)]
new <- new[,userID:=as.integer.integer64(userID)]
sick_df2 <- sick_df2[,userID:=as.integer64(userID)]
head(sick_df2)
head(healthy_df2)
sick_df2 <- read_feather("sick_df.feather") #potential alternative for exporting working space. is considerably faster, but would
head(sick_df2)
sick_df2 <- sick_df2[,userID:=as.integer64(userID)]
head(sick_df2)
sick_df2 <- sick_df2[,userID:=as.integer64.double(userID)]
sick_df2 <- data.table(sick_df2)
sick_df2 <- sick_df2[,userID:=as.integer64(userID)]
head(sick-df)
head(sick_df2)
sick_df2
sick_df2 <- data.frame(sick_df)
str(sick_df)
str(sick_df2)
head(sick_df2)
tail(sick_df2)
str(sick_df2)
sick_df2 <- read_feather("sick_df.feather") #potential alternative for exporting working space. is considerably faster, but would need some additional tweaking
str(sick_df2)
sick_df2 <- sick_df[,userID:=as.integer64(userID)]
str(sick_df2)
sick_df2
head(sick_df2)
library("profvis")
profvis({
load(file="Twitter_datatables.RData") #if the code above has been executed once, you can uncomment it and start directly from here
})
profvis({
sick_df2 <- read_feather("sick_df.feather") #potential alternative for exporting working space. is considerably faster, but would need some additional tweaking
healthy_df2 <- read_feather("healthy_df.feather")})
load(file="Twitter_datatables.RData") #use if you decided to export the whole working space
write_feather(sick_df, "sick_df.feather") #faster than save.image (but shouldn't be used for long-term storag)
write_feather(healthy_df,"healthy_df.feather") #faster than save.image
rm(list=ls())
# # LOADING and MERGING DATA FRAMES  ------
#
#   setwd(root_path) # setting WD
#   #function to make  selection of datatable based on pre_set coordinates
#
#   #loading files from sick patients
#   #see http://stackoverflow.com/questions/11433432/importing-multiple-csv-files-into-r for explanation about reading several csv-files at once
#   setwd("C:/Users/DrosoNeuro/Dropbox/UZH_Master/Masterarbeit/TwitterData/tweets_from_todd/csv_files/sick_csv") # temporarily set WD to folder with files from healthy Twitter users
#
#   temp = list.files(pattern="*.csv") #read names of all .csv files
#
#   #creates names from csv-files in folder;
#   names <- setNames(temp, make.names(gsub("*.csv$", "", temp))) #gsub uses regex to replace the specified patterns within a name
#
#   #loading df into environment
#   list2env(lapply(names,fread, header=FALSE), envir = .GlobalEnv) #"fread" reads in the data from csv
#
#   #create a list of all the datatables
#   sick_list <- lapply(attr(names,"names"),get)
#
#   #combine into a single datatable
#   sick_df <- do.call("rbind",sick_list)
#
#   remove(list = attr(names,"names"))#removing single df to save RAM
#   remove(sick_list)#removing sick_list to save RAM
#
#   col_names <- c('userID','longitude','latitude','time','sick','state')
#   colnames(sick_df) <- col_names
#   setkeyv(sick_df,col_names)
#   alarm()
#
#   #loading data from healthy Twitter users
#   setwd("C:/Users/DrosoNeuro/Dropbox/UZH_Master/Masterarbeit/TwitterData/tweets_from_todd/csv_files/one_hundred_csv") # temporarily set WD to folder with files from healthy Twitter users
#   temp = list.files(pattern="*.csv") #read names of all .csv files
#
#   #creates names from csv-files in folder;
#   names <- setNames(temp, make.names(gsub("*.csv$", "", temp))) #gsub uses regex to replace the specified patterns within a name
#
#   #loading df into environment
#   list2env(lapply(names,fread, header=FALSE), envir = .GlobalEnv)
#
#   #create a list of all the datatables
#   healthy_list <- lapply(attr(names,"names"),get)
#
#   #combine into a single datatable
#   healthy_df <- do.call("rbind",healthy_list)
#
#   remove(list = attr(names,"names"))#removing single df to save RAM
#   remove(healthy_list)#removing sick_list to save RAM
#   remove(list= c("names","temp"))
#
#   colnames(healthy_df) <- col_names
#   setkeyv(healthy_df, col_names) #sets key to column "userID"
#   remove(col_names)
#   alarm()
#
#   setwd(root_path) # set WD back
#
#   save.image(file="Twitter_datatables.RData") #saving loaded datatable to prevent loading it from the excel-files the next time
#   #fwrite(healthy_df,"healthy_df.csv") #fwrite needs developmental package of "data.table" for now (as of 2016.09.16)
#   #fwrite(sick_df,"sick_df.csv") #doesn't work yet!!! and isn't faster than simple export of data with feather!
#   #write_feather(sick_df, "sick_df.feather") #faster than save.image, bute uses more disk space (but shouldn't be used for long-term storage)
#   #write_feather(healthy_df,"healthy_df.feather") #faster than save.image, but uses more disk space
# EXPLORATORY DATA ANALYSIS ------
#if the code above has been executed once, you can uncomment it and start directly from here
setwd(root_path) # set WD back
load(file="Twitter_datatables.RData") #use if you decided to export the whole working space
# sick_df <- read_feather("sick_df.feather") #potential alternative for exporting working space. is considerably faster, but would need some additional tweaking
# sick_df <- sick_df[,userID:=as.integer64(userID)] #transform to integer64 for readability
# healthy_df <- read_feather("healthy_df.feather")})
# healthy_df <- healthy_df[,userID:=as.integer64(userID)] #transform to integer64 for readability
title_plot<-"All tweets from " #generic title plot used in some functions
# to_analyse <- "healthy_df"
# rm(list=setdiff(ls(), to_analyse)) #removes all entries from workspace except for the datatable that shall be analysed
#
#funtion to make selection of datatable based on coordinate (lon_west,lon_est,lat_south,lat_north)
coord_selection  <- function(datatable,coord_selec)
{
selec <- datatable[datatable[,longitude >=coord_selec[1] & longitude <= coord_selec[2] & latitude >= coord_selec[3] & latitude <=coord_selec[4]],]
#selec <- datatable[which(datatable[,"longitude",]>=coord_selec[1] & datatable[,"longitude"] <= coord_selec[2] & datatable[,"latitude"] >= coord_selec[3] & datatable[,"latitude"] <= coord_selec[4]),] #old way to do it with dataframes
}
#function to make selection of datatable based on coordinate (lon_west,lon_est,lat_south,lat_north); also returns index
coord_selection2  <- function(datatable,coord_selec) #
{
selec <- datatable[datatable[,longitude >=coord_selec[1] & longitude <= coord_selec[2] & latitude >= coord_selec[3] & latitude <=coord_selec[4]],]
index <-datatable[,longitude >=coord_selec[1] & longitude <= coord_selec[2] & latitude >= coord_selec[3] & latitude <=coord_selec[4]]
return(list(selec,index))
}
coord_USA <- c(-125,-66,25,50) #select only tweets from mainland USA
sick_df <- coord_selection(sick_df, coord_USA)
healthy_df <- coord_selection(healthy_df,coord_USA)
explore_data <- function(datatable,sickness_state){ #"sickness_state" takes values "sick" or "healthy" and signifies the state that the users represented in the dataste *should* be in
all_users<-unique(datatable[,userID]) #unique returns a vector, data frame or array like x but with duplicate elements/rows removed; in this case = unique return of user_ID
num_users <- length(all_users)
sick_position <- which(datatable[,sick]==1) #gets position of tweets labelled as asick
num_sick_tweets<-sum(datatable[,sick]==1) #returns number of tweets that are labelled as "sick"
#sick_tweets<-datatable[sick_position,5] #returns entries that are labelled as sick
# NB! n sick tweets != n sick users!!!
sick_users<-unique(datatable[sick_position,userID])
num_sick_users <- length(sick_users)
#getting healthy
healthy_position <- which(datatable[,sick]==0)
#healthy_tweets <- unique(datatable[healthy_position,5]) #returns entries that are labelled as healthy
num_healthy_tweets <- sum(datatable[,sick]==0)
healthy_users <- unique(datatable[healthy_position,userID]) #getting healthy users
num_healthy_users <- length(healthy_users)
#check the total number of false labels
if (sickness_state == "sick"){ #checking whether there are any users in a "sick" datatable that have never been sick, i.e. that healthy_users that don't show up in sick_users
false_label <- healthy_users[!(healthy_users %in% sick_users)]
num_false_label <- length(false_label)
}
else if (sickness_state == "healthy"){
false_label <- sick_users
num_false_label <- num_sick_users
}
out <- list(all_users,num_users,sick_position,num_sick_tweets,sick_users,num_sick_users,healthy_position,num_healthy_tweets,healthy_users,num_healthy_users,false_label,num_false_label)
names(out) <- c("all_users","num_users", "sick_position","num_sick_tweets","sick_users","num_sick_users","healthy_position", "num_healthy_tweets", "healthy_users","num_healthy_users","false_label","num_false_label")
return(out)
}
#get preliminary info from datatables
explore_sick <- explore_data(sick_df,"sick")
str(explore_sick)
explore_sick <- list(explore_sick$false_label) #prune list to save memory
names(explore_sick) <- "false_label"
explore_healthy <- explore_data(healthy_df,"healthy")
str(explore_healthy)
explore_healthy <- list(explore_healthy$false_label) #reduce size of list to save memory
names(explore_healthy) <- "false_label"
datatable <- sick_df
tag <- "sick_df"
epxlore <- explore_sick
setkey(datatable,"userID")
datatable <- datatable[time!=0,] #removing all entries which don't have a system time
datatable[,time:=as.POSIXct(datatable[,time],origin="1970-01-01")] #transforming time from system time to calendar time
datatable[,date:=as.Date(time,format="%Y-%m-%d")] #stripping exact time information
dates <- datatable[,.N,by=.(date)] #".N" is a shortcut for length(current_object), in this case, it outputs the nunber of occurences of each time-stamp in the column "time"; .() is a shorthand for "list"
to_export <- copy(datatable)
to_export <- to_export[,.(userID,longitude,latitude,time)]tz.tzNameAt(40.7271, -73.98, forceTZ=True)
to_export <- copy(datatable)
to_export <- to_export[,.(userID,longitude,latitude,time)]tz.tzNameAt(40.7271, -73.98, forceTZ=True)
to_export <- to_export[,.(userID,longitude,latitude,time)]
write_feather(to_export,"/temporary/to_export.feather")
help(write_feather)
write_feather(to_export,"/temporary/to_export.feather")
getwd()
p
write_feather(to_export,"/temporary/to_export.feather")
#function to plot number of tweets per day over time
write_feather(to_export,"/temporary/to_export.feather")
write_feather(to_export,"temporary/to_export.feather")
write_feather(to_export,"temporary/to_export.feather")
write_feather(to_export,"temporary/to_export.feather")
write_feather(to_export,"temporary/to_export.feather")
file.remove("temporary/to_export.feather")
write_feather(to_export,"temporary/to_export.feather") #save data in feather.file for export to python
to_export2 <- read_feather("temporary/to_export.feather")
to_export2
to_export2 <- data.table(to_export2)
to_export2
to_export <- to_export[,.(longitude,latitude)]
write_feather(to_export,"temporary/to_export.feather") #save data in feather.file for export to python
getwd()
to_export2 <- read_feather("temporary/to_export.feather")
to_export2 <- data.table(to_export2)
to_export2
to_export <- copy(datatable)
to_export <- to_export[,.(longitude,latitude)]
to_export
write_feather(to_export,"temporary/to_export.feather") #save data in feather.file for export to python
to_export2 <- read_feather("temporary/to_export.feather")
to_export2
to_export
write_feather(to_export,"temporary/to_export.feather") #save data in feather.file for export to python
to_export2 <- read_feather("temporary/to_export.feather")
to_export2
help(write.feather)
help(write_feather)
remove(to_export2)
to_export2 <- read_feather("temporary/to_export.feather")
to_export2
write_feather(to_export,"temporary/to_export.feather") #save data in feather.file for export to python
to_export2 <- read_feather("temporary/to_export.feather")
to_export2
