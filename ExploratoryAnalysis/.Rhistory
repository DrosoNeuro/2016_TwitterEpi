crit_val1df <- 3.84
crit_val2df <- 5.99
pmm <- matrix(c(10,35,20,8,20,30,22,8.5,2.3,9,1.5,3),4,4)
pt <- matrix(c(5,6,10,8,19,22,10,7.5,15,5,18,19,2.3,2,5,3,4,7,0.5,1.25,3.5,2,5,8.5),4,4)
pmm
pt
pmm <- dataframe(matrix(c(10,35,20,8,20,30,22,8.5,2.3,9,1.5,3),4,4))
pt <- dataframe(matrix(c(5,6,10,8,19,22,10,7.5,15,5,18,19,2.3,2,5,3,4,7,0.5,1.25,3.5,2,5,8.5),4,4))
pmm <- data.frame(matrix(c(10,35,20,8,20,30,22,8.5,2.3,9,1.5,3),4,4))
pt <- data.frame(matrix(c(5,6,10,8,19,22,10,7.5,15,5,18,19,2.3,2,5,3,4,7,0.5,1.25,3.5,2,5,8.5),4,4))
pmm$time <- pmm[,1]-pmm[,2]
pmm$cost <- pmm[,3]-pmm[,4]
pt$time <- pt[,1]-pt[,2]
pt$time <- pt[,3]-pt[,4]
pmm
ptt
pt
pt$time <- pt[,1]-pt[,2]
pt$cost <- pt[,3]-pt[,4]
pt
pt
pmm <- data.frame(matrix(c(10,35,20,8,20,30,22,8.5,2.3,9,1.5,3),4,4))
pt <- data.frame(matrix(c(5,6,10,8,19,22,10,7.5,15,5,18,19,2.3,2,5,3,4,7,0.5,1.25,3.5,2,5,8.5),6,4))
pmm$time <- pmm[,1]-pmm[,2]
pmm$cost <- pmm[,3]-pmm[,4]
pt$time <- pt[,1]-pt[,2]
pt$cost <- pt[,3]-pt[,4]
pt
plot(pt$time,pt$cost)
plot(pt$time,pt$cost,"ro")
help(plot)
plot(pt$time,pt$cost,xlim=c(-15,15),ylim=c(-6,6),"p")
plot(pt$time,pt$cost,xlim=c(-15,15),ylim=c(-6,6),"p",col="r")
plot(pt$time,pt$cost,xlim=c(-15,15),ylim=c(-6,6),"p",color="r")
plot(pt$time,pt$cost,xlim=c(-15,15),ylim=c(-6,6),pch=0)
warnings()
plot(pt$time,pt$cost,xlim=c(-15,15),ylim=c(-6,6),pch=0,col=1)
plot(pt$time,pt$cost,xlim=c(-15,15),ylim=c(-6,6),pch=0,col=0)
plot(pt$time,pt$cost,xlim=c(-15,15),ylim=c(-6,6),pch=0,col=2)
points(pmm$time,pmm$cost,pch=2,col=3)
pmm$time
pmm$cost
pmm
pmm <- data.frame(matrix(c(10,35,20,8,20,30,22,8.5,2.3,9,1.5,3),4,4))
pmm
pmm <- data.frame(matrix(c(10,35,20,8,20,30,22,8.5,2.3,9,1.5,3,1,12,2,9),4,4))
pt <- data.frame(matrix(c(5,6,10,8,19,22,10,7.5,15,5,18,19,2.3,2,5,3,4,7,0.5,1.25,3.5,2,5,8.5),6,4))
pmm$time <- pmm[,1]-pmm[,2]
pmm$cost <- pmm[,3]-pmm[,4]
pt$time <- pt[,1]-pt[,2]
pt$cost <- pt[,3]-pt[,4]
plot(pt$time,pt$cost,xlim=c(-15,15),ylim=c(-6,6),pch=0,col=2)
points(pmm$time,pmm$cost,pch=2,col=3)
trend <- function(x, beta)
{
y <- beta*x
}
help(lines)
x <- seq(-15,15)
x
trend <- function(x, beta)
{
y <- beta*x
}
beta <- -0.2630
x <- seq(-15,15)
y <- trend(x,beta)
y
x[-1]
x[1]
tail(x,1)
lines(c(x[1],y[1]),c(tail(x,1),tail(y,1)))
tail(y,1)
tail(x,1)
lines(c(x[1],tail(x,1)),c(head(y,1),tail(y,1)))
pmm <- data.frame(matrix(c(10,35,20,8,20,30,22,8.5,2.3,9,1.5,3,1,12,2,9),4,4))
pt <- data.frame(matrix(c(5,6,10,8,19,22,10,7.5,15,5,18,19,2.3,2,5,3,4,7,0.5,1.25,3.5,2,5,8.5),6,4))
pmm$time <- pmm[,1]-pmm[,2]
pmm$cost <- pmm[,3]-pmm[,4]
pt$time <- pt[,1]-pt[,2]
pt$cost <- pt[,3]-pt[,4]
plot(pt$time,pt$cost,xlim=c(-15,15),ylim=c(-6,6),pch=0,col=2)
points(pmm$time,pmm$cost,pch=2,col=3)
trend <- function(x, beta)
{
y <- beta*x
}
beta <- -0.2630
x <- seq(-15,15)
y <- trend(x,beta)
y
lines(c(x[1],tail(x,1)),c(head(y,1),tail(y,1)))
800 +600+500+600
2500*7
a <- matrix(c(15,200,50,450,135,150),2,3)
b  <- matrix(c(30,185,110,390,70,225),2,3)
c <- matrix(c(19,81,30,70,35,65),2,3)
b <- c
p1 <- b[1,1]/(b[1,1]+b[2,1])
p2 <- b[1,2]/(b[1,2]+b[2,2])
p3 <- b[1,3]/(b[1,3]+b[2,3])
p4 <- (b[1,1]+b[1,2])/(b[1,2]+b[1,1]+b[2,2]+b[2,1])
p <- (b[1,1]+b[1,2]+b[1,3])/sum(b)
l1 <-b[1,1]*log(p1)+b[2,1]*log(1-p1) + b[1,2]*log(p2)+b[2,2]*log(1-p2) + b[1,3]*log(p3)+b[2,3]*log(1-p3)
l2 <-(b[1,1]+b[1,2])*log(p4)+(b[2,1]+b[2,2])*log(1-p4) + b[1,3]*log(p3)+b[2,3]*log(1-p3)
l3 <- sum(b[1,])*log(p) + sum(b[2,])*log(1-p)
t1 <- -2*(l2-l1) #comparing unrestricted model with model2 in which medium and low income are grouped together (1 df)
t1
t2 <- -2*(l3-l2) #comparing model in which medium and low income are grouped together with completely restricted model (no income classes) (1 df)
t2
t3 <- -2*(l3-l1) #compartin unrestricted model with completely restricted model (i.e. 2 df)
t3
crit_val1df <- 3.84
crit_val2df <- 5.99
250'000/16
)
'
250000/16
15600/9
317*260
317*360
p_eva <- exp(72/100)/(exp(72/100)+exp(834/1000))
p_eva
1-2/100*18-3.75/100*2.1+0.5
p_eva <- exp(72/100)/(exp(72/100)+exp(1.06125))
p-eva
p_eva
-2/100*18-3.75/100*2.1+0.5
p_eva <- exp(72/100)/(exp(72/100)+exp(0.06125))
p_eva
m_binlog <- function(t1,c1,t2,c2,inc,univ)
{
vc <- 1-3/100*t1-6/100*c1+0.5*inc
vt <- -2/100*t2-3.75/100*c2+0.5*univ
p <- exp(vc)/(exp(vc)+exp(vt))
}
p_eva2 <- m_binlog(22,18,2,2.1,1,1)
p_eva2
eva2
p_eva
m_binlog <- function(t1,t2,c1,c2,inc,univ)
{
vc <- 1-3/100*t1-6/100*c1+0.5*inc
vt <- -2/100*t2-3.75/100*c2+0.5*univ
p <- exp(vc)/(exp(vc)+exp(vt))
return(c(vc,vt,p))
}
p_eva2 <- m_binlog(22,18,2,2.1,1,1)
p_eva2
m_binlog <- function(t1,t2,c1,c2,inc,univ)
{
vc <- 1-3/100*t1-6/100*c1+0.5*inc
vt <- -2/100*t2-3.75/100*c2+0.5*univ
p <- exp(vc)/(exp(vc)+exp(vt))
return(p)
}
p_eva2 <- m_binlog(22,18,2,2.1,1,1)
p_eva2
p_matth <- m_binlog(120,100,10,15,0,1)
p_michel <- m_binlog(10,50,3,5,1,0)
p_meri <- m_binlog(25,9,7,2.1,0,0)
p_eva
p_matth
p_meri
p_michel
help(time_zone_converter)
rm(list=ls())
library("gridExtra") #for saving png files in a specific order into pdf
library("ggplot2")
library('ggdendro')
# library("TSclust")
library("ggmap") #used to plot maps
library("maps")
library("scales") # for function alpha()
library("compiler")  # to speed up the computations!
library("plyr")
library("hexbin") #for hexoganal binning
library("rgeos") #for creating maps
library("png") #for reading png files
library("grid") #for arranging png files
library("data.table") #for faster creation of crosstables from data set & for faster searches of datatables; brings about a lot of speed-up! https://github.com/Rdatatable/data.table/wiki/Getting-started
library("bit64") #for loading data with fread
library("lubridate") #for handling time and date information; http://stackoverflow.com/questions/10705328/extract-hours-and-seconds-from-posixct-for-plotting-purposes-in-r
#install_github("rundel/timezone") #needs terminal commands: http://stackoverflow.com/questions/33381421/how-to-upgrade-proj4-for-rgdal
#sudo apt-get install libgdal-dev libproj-dev
library("timezone") #for getting timezones from lat/long data
library("feather") #for fast exporting and importing of data: http://blog.revolutionanalytics.com/2016/05/feather-package.html
#other possibility for fast exporting data is fwrite() using the data.table package: http://blog.h2o.ai/2016/04/fast-csv-writing-for-r/
root_path <- "~/Dropbox/UZH_Master/Masterarbeit/TwitterEpi/ExploratoryAnalysis" # defining root_path containing all relevant documents
script_path <- "~/Dropbox/UZH_Master/Masterarbeit/TwitterEpi/Non_R_Code"
setwd(root_path) # set WD back
load(file="Twitter_datatables2.RData") #use if you decided to export the whole working space
# sick_df <- read_feather("sick_df.feather") #potential alternative for exporting working space. is considerably faster, but would need some additional tweaking
# sick_df <- sick_df[,userID:=as.integer64(userID)] #transform to integer64 for readability
# healthy_df <- read_feather("healthy_df.feather")})
# healthy_df <- healthy_df[,userID:=as.integer64(userID)] #transform to integer64 for readability
title_plot<-"All tweets from " #generic title plot used in some functions
# to_analyse <- "healthy_df"
# rm(list=setdiff(ls(), to_analyse)) #removes all entries from workspace except for the datatable that shall be analysed
#
#funtion to make selection of datatable based on coordinate (lon_west,lon_est,lat_south,lat_north)
coord_selection  <- function(datatable,coord_selec) {
selec <- datatable[datatable[,longitude >=coord_selec[1] & longitude <= coord_selec[2] & latitude >= coord_selec[3] & latitude <=coord_selec[4]],]
#selec <- datatable[which(datatable[,"longitude",]>=coord_selec[1] & datatable[,"longitude"] <= coord_selec[2] & datatable[,"latitude"] >= coord_selec[3] & datatable[,"latitude"] <= coord_selec[4]),] #old way to do it with dataframes
}
#function to make selection of datatable based on coordinate (lon_west,lon_est,lat_south,lat_north); also returns index
coord_selection2  <- function(datatable,coord_selec) #
{
selec <- datatable[datatable[,longitude >=coord_selec[1] & longitude <= coord_selec[2] & latitude >= coord_selec[3] & latitude <=coord_selec[4]],]
index <-datatable[,longitude >=coord_selec[1] & longitude <= coord_selec[2] & latitude >= coord_selec[3] & latitude <=coord_selec[4]]
return(list(selec,index))
}
coord_USA <- c(-125,-66,25,50) #select only tweets from mainland USA
df <- coord_selection(df,coord_USA)
explore_data <- function(datatable,sickness_state){ #"sickness_state" takes values "sick" or "healthy" and signifies the state that the users represented in the dataste *should* be in
all_users<-unique(datatable[,userID]) #unique returns a vector, data frame or array like x but with duplicate elements/rows removed; in this case = unique return of user_ID
num_users <- length(all_users)
sick_position <- which(datatable[,sick]==1) #gets position of tweets labelled as asick
num_sick_tweets<-sum(datatable[,sick]==1) #returns number of tweets that are labelled as "sick"
#sick_tweets<-datatable[sick_position,5] #returns entries that are labelled as sick
# NB! n sick tweets != n sick users!!!
sick_users<-unique(datatable[sick_position,userID])
num_sick_users <- length(sick_users)
#getting healthy
healthy_position <- which(datatable[,sick]==0)
#healthy_tweets <- unique(datatable[healthy_position,5]) #returns entries that are labelled as healthy
num_healthy_tweets <- sum(datatable[,sick]==0)
healthy_users <- unique(datatable[healthy_position,userID]) #getting healthy users
num_healthy_users <- length(healthy_users)
#check the total number of false labels
if (sickness_state == "sick"){ #checking whether there are any users in a "sick" datatable that have never been sick, i.e. that healthy_users that don't show up in sick_users
false_label <- healthy_users[!(healthy_users %in% sick_users)]
num_false_label <- length(false_label)
}
else if (sickness_state == "healthy"){
false_label <- sick_users
num_false_label <- num_sick_users
}
out <- list(all_users,num_users,sick_position,num_sick_tweets,sick_users,num_sick_users,healthy_position,num_healthy_tweets,healthy_users,num_healthy_users,false_label,num_false_label)
names(out) <- c("all_users","num_users", "sick_position","num_sick_tweets","sick_users","num_sick_users","healthy_position", "num_healthy_tweets", "healthy_users","num_healthy_users","false_label","num_false_label")
return(out)
}
#get preliminary info from datatables
explore_df <- explore_data(df,"sick")
str(explore_df)
explore_df <- list(explore_df$false_label) #prune list to save memory
names(explore_df) <- "false_label"
gc()
help(scale_x_discrete)
help("scale_x_continuous")
user_activity <- function(datatable,tag){#datatable has to be in the form of a data.table; preferentially with key already set to "userID"
setkey(datatable,"userID")
user_ac <- datatable[,.N,by=.(userID)] #".N" is a shortcut for length(current_object), in this case, it outputs the nunber of occurences of each user in the column userID; .() is a shorthand for "list"
filenames <- paste0("plots/","user_activity_",tag,".pdf")
pdf(file=filenames,width=20)
#create histogram & density plot using raw counts
activity_plot <- ggplot(data =  user_ac, aes(x = user_ac[,N]))+
geom_histogram(aes(y=..density..), colour="black",fill="white",binwidth=1) + geom_density(alpha=.2, fill="#FF6666") +ggtitle(paste0('user activity_',tag))+
+scale_x_continuous(limits=c(0,200)) xlab('numb. of tweets') + ylab("proportion of users")   # Overlay with transparent density plot
print(activity_plot)
dev.off()
}
user_activity <- function(datatable,tag){#datatable has to be in the form of a data.table; preferentially with key already set to "userID"
setkey(datatable,"userID")
user_ac <- datatable[,.N,by=.(userID)] #".N" is a shortcut for length(current_object), in this case, it outputs the nunber of occurences of each user in the column userID; .() is a shorthand for "list"
filenames <- paste0("plots/","user_activity_",tag,".pdf")
pdf(file=filenames,width=20)
#create histogram & density plot using raw counts
activity_plot <- ggplot(data =  user_ac, aes(x = user_ac[,N]))+
geom_histogram(aes(y=..density..), colour="black",fill="white",binwidth=1) + geom_density(alpha=.2, fill="#FF6666") +ggtitle(paste0('user activity_',tag))+
+scale_x_continuous(limits=c(0,200))+ xlab('numb. of tweets') + ylab("proportion of users")   # Overlay with transparent density plot
print(activity_plot)
dev.off()
}
user_activity(df,"all_tweets")
c(0,200)
user_activity <- function(datatable,tag){#datatable has to be in the form of a data.table; preferentially with key already set to "userID"
setkey(datatable,"userID")
user_ac <- datatable[,.N,by=.(userID)] #".N" is a shortcut for length(current_object), in this case, it outputs the nunber of occurences of each user in the column userID; .() is a shorthand for "list"
filenames <- paste0("plots/","user_activity_",tag,".pdf")
pdf(file=filenames,width=20)
#create histogram & density plot using raw counts
activity_plot <- ggplot(data =  user_ac, aes(x = user_ac[,N]))+
geom_histogram(aes(y=..density..), colour="black",fill="white",binwidth=1) + geom_density(alpha=.2, fill="#FF6666") +ggtitle(paste0('user activity_',tag))+
+ scale_x_continuous(limits=c(0,200))+ xlab('numb. of tweets') + ylab("proportion of users")   # Overlay with transparent density plot
print(activity_plot)
dev.off()
}
user_activity(df,"all_tweets")
user_activity <- function(datatable,tag){#datatable has to be in the form of a data.table; preferentially with key already set to "userID"
setkey(datatable,"userID")
user_ac <- datatable[,.N,by=.(userID)] #".N" is a shortcut for length(current_object), in this case, it outputs the nunber of occurences of each user in the column userID; .() is a shorthand for "list"
filenames <- paste0("plots/","user_activity_",tag,".pdf")
pdf(file=filenames,width=20)
#create histogram & density plot using raw counts
activity_plot <- ggplot(data =  user_ac, aes(x = user_ac[,N]))+ scale_x_continuous(limits=c(0,200))+
geom_histogram(aes(y=..density..), colour="black",fill="white",binwidth=1) + geom_density(alpha=.2, fill="#FF6666") +ggtitle(paste0('user activity_',tag))+
+ xlab('numb. of tweets') + ylab("proportion of users")   # Overlay with transparent density plot
print(activity_plot)
dev.off()
}
user_activity(df,"all_tweets")
user_activity <- function(datatable,tag){#datatable has to be in the form of a data.table; preferentially with key already set to "userID"
setkey(datatable,"userID")
user_ac <- datatable[,.N,by=.(userID)] #".N" is a shortcut for length(current_object), in this case, it outputs the nunber of occurences of each user in the column userID; .() is a shorthand for "list"
filenames <- paste0("plots/","user_activity_",tag,".pdf")
pdf(file=filenames,width=20)
#create histogram & density plot using raw counts
activity_plot <- ggplot(data =  user_ac, aes(x = user_ac[,N]))+ scale_x_continuous(limits=c(0,200))+
geom_histogram(aes(y=..density..), colour="black",fill="white",binwidth=1) + geom_density(alpha=.2, fill="#FF6666") +ggtitle(paste0('user activity_',tag))
+ xlab('numb. of tweets') + ylab("proportion of users")   # Overlay with transparent density plot
print(activity_plot)
dev.off()
}
user_activity(df,"all_tweets")
user_activity <- function(datatable,tag){#datatable has to be in the form of a data.table; preferentially with key already set to "userID"
setkey(datatable,"userID")
user_ac <- datatable[,.N,by=.(userID)] #".N" is a shortcut for length(current_object), in this case, it outputs the nunber of occurences of each user in the column userID; .() is a shorthand for "list"
filenames <- paste0("plots/","user_activity_",tag,".pdf")
pdf(file=filenames,width=20)
#create histogram & density plot using raw counts
activity_plot <- ggplot(data =  user_ac, aes(x = user_ac[,N]))+ scale_x_continuous(limits=c(0,200))+
geom_histogram(aes(y=..density..), colour="black",fill="white",binwidth=1) + geom_density(alpha=.2, fill="#FF6666") +ggtitle(paste0('user activity_',tag)) +
xlab('numb. of tweets') + ylab("proportion of users")   # Overlay with transparent density plot
print(activity_plot)
dev.off()
}
user_activity(df,"all_tweets")
plot_hourly_activity <- function(datatable,tag,explore) {
setkey(datatable,"userID")
datatable <- datatable[time!=0,] #removing all entries which don't have a system time
#getting information on time zones that tweets where sent in; for an overlook over timezone lookup, see: http://stackoverflow.com/questions/16086962/how-to-get-a-time-zone-from-a-location-using-latitude-and-longitude-coordinates
to_export <- copy(datatable)
write_feather(to_export,"temporary/to_export.feather") #save data in feather.file for export to python
#calling python bash file in order to exectue python-based TimeZoneLookUp
#not yet fixed, so it just pauses and gives you the opportunity to call the python script
#system2("bash",paste0(script_path,"/TimeZoneFinder/TimeZoneLookUp.sh"))
#system(paste0("bash ",paste0(script_path, "/TimeZoneFinder/TimeZoneLookUp.sh")))
readkey()
to_import <- read_feather("temporary/to_import.feather") #imports processed dataset with timezones back into R
to_import <- data.table(to_import)
file.remove(c("temporary/to_export.feather","temporary/to_import.feather")) #removing .feather files
datatable[,timezone:=to_import] #add timezones to datatable
#the pytzwhere-package used to assign the timezones is not entirely accurate (see: https://github.com/mattbornski/tzwhere/issues/8)
#hence, we must handle quite a few "NA" entries; the following code offers a "quick and dirty"-solution
#get table of timezones to see how many different timezones there are
zones_before <- datatable[,.N,by=.(timezone)]
#position of NAs in timezone column
pos_NA <- is.na(datatable[,timezone])
#rough assignment of timezones based on longitude for those entries that came back as NA;
#if tweet was sent from position west of San Diego, we assign the time zone for LA;
#if tweet was sent from position east of Talahassee, we assign the time zone for NY;
#San Diego 32.8242404,-117.3753518
#Talahassee 30.4670648,-84.3969416
datatable <- datatable[pos_NA,timezone:=ifelse(longitude<=-117,"America/Los_Angeles",
ifelse(longitude>=-84,"America/New_York",timezone))]
#get table of timezones to see how many NAs are left (usually, the above fix gets rid of over 90% of NA)
zones_after <- datatable[,.N,by=.(timezone)]
#removing all remaining NAs
datatable <- datatable[!is.na(timezone),]
##transforming system time to calendar time
datatable[,time1:=as.POSIXct(datatable[,time],origin="1970-01-01",tz="UTC")] #transforming time from system time to calendar time UTC
date_converter <- function(datatable){
zones <- datatable[,.N,by=.(timezone)] #extracting timezones
for (x in zones$timezone) #looping through each timezone
{
#converts times for tweets sent in respective timezone
datatable[timezone==x,time2:=as.POSIXct(datatable[timezone==x,time],origin="1970-01-01",tz=x)]
}
}
date_converter(datatable)
#extracting hour and minute information and saving at as decimal hours
datatable[,hour1:=hour(datatable[,time1])+round(minute(datatable[,time1])/60,2)] #from UTC time
datatable[,hour2:=hour(datatable[,time2])+round(minute(datatable[,time2])/60,2)] #from local timezone
hours1 <- datatable[,.N,by=.(hour1)] #".N" is a shortcut for length(current_object), in this case, it outputs the nunber of occurences of each time-stamp in the column "hour1"; .() is a shorthand for "list"
hours2 <- datatable[,.N,by=.(hour2)]
#function to plot number of tweets per hour over time (at this stage only for the hour2 cases)
ts_plotter1 <- function(hours,title="Tweets") {
p <- ggplot(data=hours, aes(x=hour1,y=N)) +geom_bar(stat="identity") +
scale_x_continuous(limits=c(0,24)) +
ylab("Frequency") + xlab("hours of the day")+ggtitle(paste0(title,"_",tag))
}
ts_plotter2 <- function(hours,title="Tweets") {
p <- ggplot(data=hours, aes(x=hour2,y=N)) +geom_bar(stat="identity") +
scale_x_continuous(limits=c(0,24)) +
ylab("Frequency") + xlab("hours of the day")+ggtitle(paste0(title,"_",tag))
}
#all tweets across time
all_tweets <- list()
all_tweets[[1]] <- ts_plotter1(hours1,"All tweets") #create plot with number of tweets per day over whole time period
all_tweets[[2]]<- ts_plotter2(hours2,"All tweets")
#sick across time
hours1_sick <- datatable[sick==1,.N,by=.(hour1)] #creating a table of the number of tweets per hour that are labelled as "sick"
hours2_sick <- datatable[sick==1,.N,by=.(hour2)] #creating a table of the number of tweets per hour that are labelled as "sick"
sick_tweets <- list()
sick_tweets[[1]] <- ts_plotter1(hours1_sick,"Sick tweets")
sick_tweets[[2]] <- ts_plotter2(hours2_sick,"Sick tweets")
#healthy across time
hours1_healthy <- datatable[sick==0,.N,by=.(hour1)] #creating table with no of healthy tweets per hour
hours2_healthy <- datatable[sick==0,.N,by=.(hour2)]
healthy_tweets <- list()
healthy_tweets[[1]] <- ts_plotter1(hours1_healthy,"healthy tweets")
healthy_tweets[[2]] <- ts_plotter2(hours2_healthy,"healthy tweets")
#mislabelled tweets across time
hours1_mislabelled <- datatable[userID %in% explore$false_label,.N,by=.(hour1)]
hours2_mislabelled <- datatable[userID %in% explore$false_label,.N,by=.(hour2)]
#this plots *all* the tweets from the users that had one or more mislabelled tweets!
#if I wanted to just plot the mislabelled tweets themselves, I'd need to add "& sick==1" or "& sick ==0" for the sick_df and healthy_df, respectively
mis_tweets <- list()
mis_tweets[[1]] <- ts_plotter1(hours1_mislabelled,"mislabelled tweets")
mis_tweets[[2]] <- ts_plotter2(hours2_mislabelled,"mislabelled tweets")
#adding plots to pdf
filenames <- paste0("plots/","tweets_per_hour_",tag,".pdf")
pdf(file=filenames,width=20)
multiplot(all_tweets[[1]],sick_tweets[[1]],healthy_tweets[[1]],mis_tweets[[1]],cols=1) #plotting tweets per hour1 over whole time series
multiplot(all_tweets[[2]],sick_tweets[[2]],healthy_tweets[[2]],mis_tweets[[2]],cols=1) #plotting tweets per hour2 over whole time series
dev.off()
}
gc()
plot_hourly_activity(df,"all_tweets",explore_df)
gc()
plot_hourly_activity <- function(datatable,tag,explore) {
setkey(datatable,"userID")
datatable <- datatable[time!=0,] #removing all entries which don't have a system time
#getting information on time zones that tweets where sent in; for an overlook over timezone lookup, see: http://stackoverflow.com/questions/16086962/how-to-get-a-time-zone-from-a-location-using-latitude-and-longitude-coordinates
write_feather(datatable,"temporary/to_export.feather") #save data in feather.file for export to python
#calling python bash file in order to exectue python-based TimeZoneLookUp
#not yet fixed, so it just pauses and gives you the opportunity to call the python script
#system2("bash",paste0(script_path,"/TimeZoneFinder/TimeZoneLookUp.sh"))
#system(paste0("bash ",paste0(script_path, "/TimeZoneFinder/TimeZoneLookUp.sh")))
gc()
readkey()
to_import <- read_feather("temporary/to_import.feather") #imports processed dataset with timezones back into R
to_import <- data.table(to_import)
file.remove(c("temporary/to_export.feather","temporary/to_import.feather")) #removing .feather files
datatable[,timezone:=to_import] #add timezones to datatable
#the pytzwhere-package used to assign the timezones is not entirely accurate (see: https://github.com/mattbornski/tzwhere/issues/8)
#hence, we must handle quite a few "NA" entries; the following code offers a "quick and dirty"-solution
#get table of timezones to see how many different timezones there are
zones_before <- datatable[,.N,by=.(timezone)]
#position of NAs in timezone column
pos_NA <- is.na(datatable[,timezone])
#rough assignment of timezones based on longitude for those entries that came back as NA;
#if tweet was sent from position west of San Diego, we assign the time zone for LA;
#if tweet was sent from position east of Talahassee, we assign the time zone for NY;
#San Diego 32.8242404,-117.3753518
#Talahassee 30.4670648,-84.3969416
datatable <- datatable[pos_NA,timezone:=ifelse(longitude<=-117,"America/Los_Angeles",
ifelse(longitude>=-84,"America/New_York",timezone))]
#get table of timezones to see how many NAs are left (usually, the above fix gets rid of over 90% of NA)
zones_after <- datatable[,.N,by=.(timezone)]
#removing all remaining NAs
datatable <- datatable[!is.na(timezone),]
##transforming system time to calendar time
datatable[,time1:=as.POSIXct(datatable[,time],origin="1970-01-01",tz="UTC")] #transforming time from system time to calendar time UTC
date_converter <- function(datatable){
zones <- datatable[,.N,by=.(timezone)] #extracting timezones
for (x in zones$timezone) #looping through each timezone
{
#converts times for tweets sent in respective timezone
datatable[timezone==x,time2:=as.POSIXct(datatable[timezone==x,time],origin="1970-01-01",tz=x)]
}
}
date_converter(datatable)
#extracting hour and minute information and saving at as decimal hours
datatable[,hour1:=hour(datatable[,time1])+round(minute(datatable[,time1])/60,2)] #from UTC time
datatable[,hour2:=hour(datatable[,time2])+round(minute(datatable[,time2])/60,2)] #from local timezone
hours1 <- datatable[,.N,by=.(hour1)] #".N" is a shortcut for length(current_object), in this case, it outputs the nunber of occurences of each time-stamp in the column "hour1"; .() is a shorthand for "list"
hours2 <- datatable[,.N,by=.(hour2)]
#function to plot number of tweets per hour over time (at this stage only for the hour2 cases)
ts_plotter1 <- function(hours,title="Tweets") {
p <- ggplot(data=hours, aes(x=hour1,y=N)) +geom_bar(stat="identity") +
scale_x_continuous(limits=c(0,24)) +
ylab("Frequency") + xlab("hours of the day")+ggtitle(paste0(title,"_",tag))
}
ts_plotter2 <- function(hours,title="Tweets") {
p <- ggplot(data=hours, aes(x=hour2,y=N)) +geom_bar(stat="identity") +
scale_x_continuous(limits=c(0,24)) +
ylab("Frequency") + xlab("hours of the day")+ggtitle(paste0(title,"_",tag))
}
#all tweets across time
all_tweets <- list()
all_tweets[[1]] <- ts_plotter1(hours1,"All tweets") #create plot with number of tweets per day over whole time period
all_tweets[[2]]<- ts_plotter2(hours2,"All tweets")
#sick across time
hours1_sick <- datatable[sick==1,.N,by=.(hour1)] #creating a table of the number of tweets per hour that are labelled as "sick"
hours2_sick <- datatable[sick==1,.N,by=.(hour2)] #creating a table of the number of tweets per hour that are labelled as "sick"
sick_tweets <- list()
sick_tweets[[1]] <- ts_plotter1(hours1_sick,"Sick tweets")
sick_tweets[[2]] <- ts_plotter2(hours2_sick,"Sick tweets")
#healthy across time
hours1_healthy <- datatable[sick==0,.N,by=.(hour1)] #creating table with no of healthy tweets per hour
hours2_healthy <- datatable[sick==0,.N,by=.(hour2)]
healthy_tweets <- list()
healthy_tweets[[1]] <- ts_plotter1(hours1_healthy,"healthy tweets")
healthy_tweets[[2]] <- ts_plotter2(hours2_healthy,"healthy tweets")
#mislabelled tweets across time
hours1_mislabelled <- datatable[userID %in% explore$false_label,.N,by=.(hour1)]
hours2_mislabelled <- datatable[userID %in% explore$false_label,.N,by=.(hour2)]
#this plots *all* the tweets from the users that had one or more mislabelled tweets!
#if I wanted to just plot the mislabelled tweets themselves, I'd need to add "& sick==1" or "& sick ==0" for the sick_df and healthy_df, respectively
mis_tweets <- list()
mis_tweets[[1]] <- ts_plotter1(hours1_mislabelled,"mislabelled tweets")
mis_tweets[[2]] <- ts_plotter2(hours2_mislabelled,"mislabelled tweets")
#adding plots to pdf
filenames <- paste0("plots/","tweets_per_hour_",tag,".pdf")
pdf(file=filenames,width=20)
multiplot(all_tweets[[1]],sick_tweets[[1]],healthy_tweets[[1]],mis_tweets[[1]],cols=1) #plotting tweets per hour1 over whole time series
multiplot(all_tweets[[2]],sick_tweets[[2]],healthy_tweets[[2]],mis_tweets[[2]],cols=1) #plotting tweets per hour2 over whole time series
dev.off()
}
plot_hourly_activity(df,"all_tweets",explore_df)
