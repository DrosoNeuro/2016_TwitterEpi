y <- lapply(n, funct1, a,c)
y
n <- seq(1,100)
a <- 5
c <- 1
y <- lapply(n, funct1, a,c)
y
2*c/log(4/3)
a <- 8
c <- 1
y <- lapply(n, funct1, a,c)
funct1(1,a,c,n)
y
y Ã¼1
y[1]
funct2 <- function(n,a,c){
y <- a*log(3*n/4)-a*log((4/3)**n)
}
funct2 <- function(n,a,c){
y <- a*log(3*n/4)-a*log((4/3)**n)
y
}
funct2(n,a,c)
a <- 1
funct2(n,a,c)
log(3/4)
log(6/4)
6/4
(4/3)**2
4/3
log(2/3)
log(1/2)
log(1/2)
log(2/3)
log(1/2)
sqrt(0.69**2+(6.33e-7)**2)
sqrt(0.69**2+(6.33e-7)**2)*6371
sqrt(0.6933**2+(6.33e-7)**2)*6371
install.packages("rJava")
install.packages("rJava")
.libPaths()
library(rJava)
install.packages("RWeka",depencies=T)
install.packages("RWekajars",depencies=T)
install.packages("RWekajars",dependencies=T)
install.packages("RWeka",dependencies=T)
library(tm)
install.packages("tm")
install.packages("tm",dependencies=T)
install.packages("slam",dependencies=T)
remove(list=ls())
library("gridExtra") #for saving png files in a specific order into pdf
library("ggplot2")
library('ggdendro')
# library("TSclust")
library("ggmap") #used to plot maps
library("maps")
library("scales") # for function alpha()
library("compiler")  # to speed up the computations!
library("plyr")
library("hexbin") #for hexoganal binning
library("rgeos") #for creating maps
library("png") #for reading png files
library("grid") #for arranging png files
library("data.table") #for faster creation of crosstables from data set & for faster searches of datatables; brings about a lot of speed-up! https://github.com/Rdatatable/data.table/wiki/Getting-started
library("bit64") #for loading data with fread
library("lubridate") #for handling time and date information; http://stackoverflow.com/questions/10705328/extract-hours-and-seconds-from-posixct-for-plotting-purposes-in-r
#install.packages("devtools") > for install_github
#install_github("rundel/timezone") #needs terminal commands: http://stackoverflow.com/questions/33381421/how-to-upgrade-proj4-for-rgdal
#sudo apt-get install libgdal-dev libproj-dev
library("timezone") #for getting timezones from lat/long data
library("feather") #for fast exporting and importing of data: http://blog.revolutionanalytics.com/2016/05/feather-package.html
#other possibility for fast exporting data is fwrite() using the data.table package: http://blog.h2o.ai/2016/04/fast-csv-writing-for-r/
root_path <- "~/Dropbox/UZH_Master/Masterarbeit/TwitterEpi/ExploratoryAnalysis" # defining root_path containing all relevant documents
script_path <- "~/Dropbox/UZH_Master/Masterarbeit/TwitterEpi/Non_R_Code"
###general functions needed for plotting etc. -------------
# Multiple plot function
#http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
library(grid)
#make a list from the ... arguments and plotlist
plots <- c(list(...), plotlist)
numPlots = length(plots)
# If layout is NULL, then use 'cols' to determine layout
if (is.null(layout)) {
# Make the panel
# ncol: Number of columns of plots
# nrow: Number of rows needed, calculated from # of cols
layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
ncol = cols, nrow = ceiling(numPlots/cols))
}
if (numPlots==1) {
print(plots[[1]])
} else {
# Set up the page
grid.newpage()
pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
# Make each plot, in the correct location
for (i in 1:numPlots) {
# Get the i,j matrix positions of the regions that contain this subplot
matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
layout.pos.col = matchidx$col))
}
}
}
#function to pause the execution of another function until key is pressed
readkey <- function(){
cat ("Press [enter] to continue")
line <- readline()
}
# # NB!! the BEST WAY TO CHOOSE THE BIN IS probably ***** "FD" *****:
# # http://stats.stackexchange.com/questions/798/calculating-optimal-number-of-bins-in-a-histogram-for-n-where-n-ranges-from-30
#
# EXPLORATORY DATA ANALYSIS ------
#if the code above has been executed once, you can uncomment it and start directly from here
setwd(root_path) # set WD back
load(file="sick_tweets.RData") #use if you decided to export the whole working space
# sick_df <- read_feather("sick_df.feather") #potential alternative for exporting working space. is considerably faster, but would need some additional tweaking
# sick_df <- sick_df[,userID:=as.integer64(userID)] #transform to integer64 for readability
# healthy_df <- read_feather("healthy_df.feather")})
# healthy_df <- healthy_df[,userID:=as.integer64(userID)] #transform to integer64 for readability
df_label <- "sick"
title_plot<-"All tweets from " #generic title plot used in some functions
# to_analyse <- "healthy_df"
# rm(list=setdiff(ls(), to_analyse)) #removes all entries from workspace except for the datatable that shall be analysed
#
#funtion to make selection of datatable based on coordinate (lon_west,lon_est,lat_south,lat_north)
coord_selection  <- function(datatable,coord_selec) {
selec <- datatable[datatable[,longitude >=coord_selec[1] & longitude <= coord_selec[2] & latitude >= coord_selec[3] & latitude <=coord_selec[4]],]
#selec <- datatable[which(datatable[,"longitude",]>=coord_selec[1] & datatable[,"longitude"] <= coord_selec[2] & datatable[,"latitude"] >= coord_selec[3] & datatable[,"latitude"] <= coord_selec[4]),] #old way to do it with dataframes
}
#function to make selection of datatable based on coordinate (lon_west,lon_est,lat_south,lat_north); also returns index
coord_selection2  <- function(datatable,coord_selec) #
{
selec <- datatable[datatable[,longitude >=coord_selec[1] & longitude <= coord_selec[2] & latitude >= coord_selec[3] & latitude <=coord_selec[4]],]
index <-datatable[,longitude >=coord_selec[1] & longitude <= coord_selec[2] & latitude >= coord_selec[3] & latitude <=coord_selec[4]]
return(list(selec,index))
}
coord_USA <- c(-125,-66,25,50) #select only tweets from mainland USA
df <- coord_selection(df,coord_USA)
#sick_df <- coord_selection(sick_df, coord_USA)
#healthy_df <- coord_selection(healthy_df,coord_USA)
gc()
#function to explore basic characteristics of dataset
#still needs to be adapted to "all" tweets!
explore_data <- function(datatable,sickness_state){ #"sickness_state" takes values "sick" or "healthy" and signifies the state that the users represented in the dataste *should* be in
all_users<-unique(datatable[,userID]) #unique returns a vector, data frame or array like x but with duplicate elements/rows removed; in this case = unique return of user_ID
num_users <- length(all_users)
sick_position <- which(datatable[,sick]==1) #gets position of tweets labelled as asick
num_sick_tweets<-sum(datatable[,sick]==1) #returns number of tweets that are labelled as "sick"
#sick_tweets<-datatable[sick_position,5] #returns entries that are labelled as sick
# NB! n sick tweets != n sick users!!!
sick_users<-unique(datatable[sick_position,userID])
num_sick_users <- length(sick_users)
#getting healthy
healthy_position <- which(datatable[,sick]==0)
#healthy_tweets <- unique(datatable[healthy_position,5]) #returns entries that are labelled as healthy
num_healthy_tweets <- sum(datatable[,sick]==0)
healthy_users <- unique(datatable[healthy_position,userID]) #getting healthy users
num_healthy_users <- length(healthy_users)
#check the total number of false labels
if (sickness_state == "sick"){ #checking whether there are any users in a "sick" datatable that have never been sick, i.e. that healthy_users that don't show up in sick_users
false_label <- healthy_users[!(healthy_users %in% sick_users)]
num_false_label <- length(false_label)
}
else if (sickness_state == "healthy"){
false_label <- sick_users
num_false_label <- num_sick_users
}
#dirty hack in order to get code running for random subsets (i.e. without sick/healthy classification) > needs to be updated for final version
else{
false_label <- sick_users
num_false_label <- num_sick_users
}
out <- list(all_users,num_users,sick_position,num_sick_tweets,sick_users,num_sick_users,healthy_position,num_healthy_tweets,healthy_users,num_healthy_users,false_label,num_false_label)
names(out) <- c("all_users","num_users", "sick_position","num_sick_tweets","sick_users","num_sick_users","healthy_position", "num_healthy_tweets", "healthy_users","num_healthy_users","false_label","num_false_label")
return(out)
}
#get preliminary info from datatables
explore_df <- explore_data(df,df_label)
str(explore_df)
explore_df <- list(explore_df$false_label) #prune list to save memory
names(explore_df) <- "false_label"
gc()
plot_hourly_activity <- function(datatable,tag,explore) {
setkey(datatable,"userID")
datatable <- datatable[time!=0,] #removing all entries which don't have a system time
#getting information on time zones that tweets where sent in; for an overlook over timezone lookup, see: http://stackoverflow.com/questions/16086962/how-to-get-a-time-zone-from-a-location-using-latitude-and-longitude-coordinates
to_export <- copy(datatable)
write_feather(to_export,"temporary/to_export.feather") #save data in feather.file for export to python
#calling python bash file in order to exectue python-based TimeZoneLookUp
system2("bash",paste0(script_path,"/TimeZoneFinder/TimeZoneLookUp.sh"))
#system(paste0("bash ",paste0(script_path, "/TimeZoneFinder/TimeZoneLookUp.sh")))
#readkey()
to_import <- read_feather("temporary/to_import.feather") #imports processed dataset with timezones back into R
to_import <- data.table(to_import)
#file.remove(c("temporary/to_export.feather","temporary/to_import.feather")) #removing .feather files
datatable[,timezone:=to_import] #add timezones to datatable
#the pytzwhere-package used to assign the timezones is not entirely accurate (see: https://github.com/mattbornski/tzwhere/issues/8)
#hence, we must handle quite a few "NA" entries; the following code offers a "quick and dirty"-solution
#get table of timezones to see how many different timezones there are
zones_before <- datatable[,.N,by=.(timezone)]
#position of NAs in timezone column
pos_NA <- is.na(datatable[,timezone])
#rough assignment of timezones based on longitude for those entries that came back as NA;
#if tweet was sent from position west of San Diego, we assign the time zone for LA;
#if tweet was sent from position east of Talahassee, we assign the time zone for NY;
#San Diego 32.8242404,-117.3753518
#Talahassee 30.4670648,-84.3969416
datatable <- datatable[pos_NA,timezone:=ifelse(longitude<=-117,"America/Los_Angeles",
ifelse(longitude>=-84,"America/New_York",timezone))]
#get table of timezones to see how many NAs are left (usually, the above fix gets rid of over 90% of NA)
zones_after <- datatable[,.N,by=.(timezone)]
#removing all remaining NAs
datatable <- datatable[!is.na(timezone),]
##transforming system time to calendar time
datatable[,time1:=as.POSIXct(datatable[,time],origin="1970-01-01",tz="UTC")] #transforming time from system time to calendar time UTC
date_converter <- function(datatable){
zones <- datatable[,.N,by=.(timezone)] #extracting timezones
for (x in zones$timezone) #looping through each timezone
{
#converts times for tweets sent in respective timezone
datatable[timezone==x,time2:=as.POSIXct(datatable[timezone==x,time],origin="1970-01-01",tz=x)]
}
}
date_converter(datatable)
#extracting hour and minute information and saving at as decimal hours
datatable[,hour1:=hour(datatable[,time1])+round(minute(datatable[,time1])/60,2)] #from UTC time
datatable[,hour2:=hour(datatable[,time2])+round(minute(datatable[,time2])/60,2)] #from local timezone
hours1 <- datatable[,.N,by=.(hour1)] #".N" is a shortcut for length(current_object), in this case, it outputs the nunber of occurences of each time-stamp in the column "hour1"; .() is a shorthand for "list"
hours2 <- datatable[,.N,by=.(hour2)]
#function to plot number of tweets per hour over time (at this stage only for the hour2 cases)
ts_plotter1 <- function(hours,title="Tweets") {
p <- ggplot(data=hours, aes(x=hour1,y=N)) +geom_bar(stat="identity") +
scale_x_continuous(limits=c(0,24)) +
ylab("Frequency") + xlab("hours of the day")+ggtitle(paste0(title,"_",tag))
}
ts_plotter2 <- function(hours,title="Tweets") {
p <- ggplot(data=hours, aes(x=hour2,y=N)) +geom_bar(stat="identity") +
scale_x_continuous(limits=c(0,24)) +
ylab("Frequency") + xlab("hours of the day")+ggtitle(paste0(title,"_",tag))
}
#all tweets across time
all_tweets <- list()
all_tweets[[1]] <- ts_plotter1(hours1,"All tweets") #create plot with number of tweets per day over whole time period
all_tweets[[2]]<- ts_plotter2(hours2,"All tweets")
#sick across time
hours1_sick <- datatable[sick==1,.N,by=.(hour1)] #creating a table of the number of tweets per hour that are labelled as "sick"
hours2_sick <- datatable[sick==1,.N,by=.(hour2)] #creating a table of the number of tweets per hour that are labelled as "sick"
sick_tweets <- list()
sick_tweets[[1]] <- ts_plotter1(hours1_sick,"Sick tweets")
sick_tweets[[2]] <- ts_plotter2(hours2_sick,"Sick tweets")
#healthy across time
hours1_healthy <- datatable[sick==0,.N,by=.(hour1)] #creating table with no of healthy tweets per hour
hours2_healthy <- datatable[sick==0,.N,by=.(hour2)]
healthy_tweets <- list()
healthy_tweets[[1]] <- ts_plotter1(hours1_healthy,"healthy tweets")
healthy_tweets[[2]] <- ts_plotter2(hours2_healthy,"healthy tweets")
#mislabelled tweets across time
hours1_mislabelled <- datatable[userID %in% explore$false_label,.N,by=.(hour1)]
hours2_mislabelled <- datatable[userID %in% explore$false_label,.N,by=.(hour2)]
#this plots *all* the tweets from the users that had one or more mislabelled tweets!
#if I wanted to just plot the mislabelled tweets themselves, I'd need to add "& sick==1" or "& sick ==0" for the sick_df and healthy_df, respectively
mis_tweets <- list()
mis_tweets[[1]] <- ts_plotter1(hours1_mislabelled,"mislabelled tweets")
mis_tweets[[2]] <- ts_plotter2(hours2_mislabelled,"mislabelled tweets")
#adding plots to pdf
filenames <- paste0("plots/","tweets_per_hour_",tag,".pdf")
pdf(file=filenames,width=20)
multiplot(all_tweets[[1]],sick_tweets[[1]],healthy_tweets[[1]],mis_tweets[[1]],cols=1) #plotting tweets per hour1 over whole time series
multiplot(all_tweets[[2]],sick_tweets[[2]],healthy_tweets[[2]],mis_tweets[[2]],cols=1) #plotting tweets per hour2 over whole time series
dev.off()
}
plot_hourly_activity(df,df_label,explore_df)
##still needs some tinkering and cleaning up! (especially when it comes to automating the plotting)
plot_hourly_activity <- function(datatable,tag,explore) {
setkey(datatable,"userID")
datatable <- datatable[time!=0,] #removing all entries which don't have a system time
#getting information on time zones that tweets where sent in; for an overlook over timezone lookup, see: http://stackoverflow.com/questions/16086962/how-to-get-a-time-zone-from-a-location-using-latitude-and-longitude-coordinates
to_export <- copy(datatable)
write_feather(to_export,"temporary/to_export.feather") #save data in feather.file for export to python
#calling python bash file in order to exectue python-based TimeZoneLookUp
#system2("bash",paste0(script_path,"/TimeZoneFinder/TimeZoneLookUp.sh"))
#alternative solution: Stop script, execute python conversion and hit key to continue
#readkey()
to_import <- read_feather("temporary/to_import.feather") #imports processed dataset with timezones back into R
to_import <- data.table(to_import)
#file.remove(c("temporary/to_export.feather","temporary/to_import.feather")) #removing .feather files
datatable[,timezone:=to_import] #add timezones to datatable
#the pytzwhere-package used to assign the timezones is not entirely accurate (see: https://github.com/mattbornski/tzwhere/issues/8)
#hence, we must handle quite a few "NA" entries; the following code offers a "quick and dirty"-solution
#get table of timezones to see how many different timezones there are
zones_before <- datatable[,.N,by=.(timezone)]
#position of NAs in timezone column
pos_NA <- is.na(datatable[,timezone])
#rough assignment of timezones based on longitude for those entries that came back as NA;
#if tweet was sent from position west of San Diego, we assign the time zone for LA;
#if tweet was sent from position east of Talahassee, we assign the time zone for NY;
#San Diego 32.8242404,-117.3753518
#Talahassee 30.4670648,-84.3969416
datatable <- datatable[pos_NA,timezone:=ifelse(longitude<=-117,"America/Los_Angeles",
ifelse(longitude>=-84,"America/New_York",timezone))]
#get table of timezones to see how many NAs are left (usually, the above fix gets rid of over 90% of NA)
zones_after <- datatable[,.N,by=.(timezone)]
#removing all remaining NAs
datatable <- datatable[!is.na(timezone),]
##transforming system time to calendar time
datatable[,time1:=as.POSIXct(datatable[,time],origin="1970-01-01",tz="UTC")] #transforming time from system time to calendar time UTC
date_converter <- function(datatable){
zones <- datatable[,.N,by=.(timezone)] #extracting timezones
for (x in zones$timezone) #looping through each timezone
{
#converts times for tweets sent in respective timezone
datatable[timezone==x,time2:=as.POSIXct(datatable[timezone==x,time],origin="1970-01-01",tz=x)]
}
}
date_converter(datatable)
#extracting hour and minute information and saving at as decimal hours
datatable[,hour1:=hour(datatable[,time1])+round(minute(datatable[,time1])/60,2)] #from UTC time
datatable[,hour2:=hour(datatable[,time2])+round(minute(datatable[,time2])/60,2)] #from local timezone
hours1 <- datatable[,.N,by=.(hour1)] #".N" is a shortcut for length(current_object), in this case, it outputs the nunber of occurences of each time-stamp in the column "hour1"; .() is a shorthand for "list"
hours2 <- datatable[,.N,by=.(hour2)]
#function to plot number of tweets per hour over time (at this stage only for the hour2 cases)
ts_plotter1 <- function(hours,title="Tweets") {
p <- ggplot(data=hours, aes(x=hour1,y=N)) +geom_bar(stat="identity") +
scale_x_continuous(limits=c(0,24)) +
ylab("Frequency") + xlab("hours of the day")+ggtitle(paste0(title,"_",tag))
}
ts_plotter2 <- function(hours,title="Tweets") {
p <- ggplot(data=hours, aes(x=hour2,y=N)) +geom_bar(stat="identity") +
scale_x_continuous(limits=c(0,24)) +
ylab("Frequency") + xlab("hours of the day")+ggtitle(paste0(title,"_",tag))
}
#all tweets across time
all_tweets <- list()
all_tweets[[1]] <- ts_plotter1(hours1,"All tweets") #create plot with number of tweets per day over whole time period
all_tweets[[2]]<- ts_plotter2(hours2,"All tweets")
#sick across time
hours1_sick <- datatable[sick==1,.N,by=.(hour1)] #creating a table of the number of tweets per hour that are labelled as "sick"
hours2_sick <- datatable[sick==1,.N,by=.(hour2)] #creating a table of the number of tweets per hour that are labelled as "sick"
sick_tweets <- list()
sick_tweets[[1]] <- ts_plotter1(hours1_sick,"Sick tweets")
sick_tweets[[2]] <- ts_plotter2(hours2_sick,"Sick tweets")
#healthy across time
hours1_healthy <- datatable[sick==0,.N,by=.(hour1)] #creating table with no of healthy tweets per hour
hours2_healthy <- datatable[sick==0,.N,by=.(hour2)]
healthy_tweets <- list()
healthy_tweets[[1]] <- ts_plotter1(hours1_healthy,"healthy tweets")
healthy_tweets[[2]] <- ts_plotter2(hours2_healthy,"healthy tweets")
#mislabelled tweets across time
hours1_mislabelled <- datatable[userID %in% explore$false_label,.N,by=.(hour1)]
hours2_mislabelled <- datatable[userID %in% explore$false_label,.N,by=.(hour2)]
#this plots *all* the tweets from the users that had one or more mislabelled tweets!
#if I wanted to just plot the mislabelled tweets themselves, I'd need to add "& sick==1" or "& sick ==0" for the sick_df and healthy_df, respectively
mis_tweets <- list()
mis_tweets[[1]] <- ts_plotter1(hours1_mislabelled,"mislabelled tweets")
mis_tweets[[2]] <- ts_plotter2(hours2_mislabelled,"mislabelled tweets")
#adding plots to pdf
filenames <- paste0("plots/","tweets_per_hour_",tag,".pdf")
pdf(file=filenames,width=20)
multiplot(all_tweets[[1]],sick_tweets[[1]],healthy_tweets[[1]],mis_tweets[[1]],cols=1) #plotting tweets per hour1 over whole time series
multiplot(all_tweets[[2]],sick_tweets[[2]],healthy_tweets[[2]],mis_tweets[[2]],cols=1) #plotting tweets per hour2 over whole time series
dev.off()
}
plot_hourly_activity(df,df_label,explore_df)
df
df
explore_df <- explore_data(df,df_label)
explore_df
head(explore_df)
str(explore_df)
df_explore <- explore_data(df,df_label)
explore_df <- list(explore_df$false_label) #prune list to save memory
names(explore_df) <- "false_label"
df_explore
str(df_explore)
df[userID==32,]
df[userID==322,]
setwd(root_path) # setting WD
try(setwd("/media/drosoneuro/E230270C3026E6EF/tweet_ratings/one_hundred/parsed"), stop("no directory found"))
str(df_explore)
df_explore$num_false_label+df_explore$num_sick_uswea
df_explore$num_false_label+df_explore$num_sick_uswea
df_explore$num_false_label+df_explore$num_sick_users
setwd(root_path) # setting WD
#function to make  selection of datatable based on pre_set coordinates
#see http://stackoverflow.com/questions/11433432/importing-multiple-csv-files-into-r for explanation about reading several csv-files at once
#loading files from all patients
#try(setwd("/media/drosoneuro/E230270C3026E6EF/tweet_ratings/all_tweets/parsed/subset"), stop("no directory found")) # temporarily set WD to folder with files from healthy Twitter users
try(setwd("/media/drosoneuro/E230270C3026E6EF/tweet_ratings/one_hundred/parsed"), stop("no directory found"))
#try(setwd("/media/drosoneuro/E230270C3026E6EF/tweet_ratings/sick_users/parsed/"), stop("no directory found"))
temp = list.files(pattern="*.feather") #read names of all .feather files
ifelse(length(temp) == 0, "no feather files found",{
#creates names from feather-files in folder;
names <- setNames(temp, make.names(gsub("*.feather$", "", temp))) #gsub uses regex to replace the specified patterns within a name
#loading df into environment
list2env(lapply(names,read_feather), envir = .GlobalEnv) #"read_feather" reads in the data from feather_files
#create a list of all the datatables
data_list <- lapply(attr(names,"names"),get)
#combine into a single datatable
#Implement automated stepwise binding! e.g. by using length(data_list)%10 to calculate number of iterations
iter <- length(data_list)%/%5
remainder <- length(data_list)%%5
for (i in 0:iter){
if (i==0){
df <- do.call("rbind",tail(data_list,remainder))
df <- data.table(df)
remove(list = attr(tail(names,remainder),"names"))#removing single df to save RAM
gc()
}
else{
start = 1+(i-1)*5
end = 5+(i-1)*5
df_temp <- do.call("rbind",data_list[start:end])
df_temp <- data.table(df_temp)
remove(list = attr(names[start:end],"names"))#removing single df to save RAM
gc()
df <- rbind(df,df_temp)
remove(df_temp)
gc()
}
}
col_names <- c('userID','longitude','latitude','time','sick','state')
colnames(df) <- col_names
setkeyv(df,col_names)
rm(list=setdiff(ls(), c("df","root_path","script_path"))) #removes all files except for df
##loading all files from "sick" users
# #setwd("C:/Users/DrosoNeuro/Dropbox/UZH_Master/Masterarbeit/TwitterData/tweets_from_todd/csv_files/sick_csv") # temporarily set WD to folder with files from healthy Twitter users
# setwd("C:/Users/DrosoNeuro/Dropbox/UZH_Master/Masterarbeit/TwitterData/tweets_from_todd/csv_files/sick_csv") # temporarily set WD to folder with files from healthy Twitter users
# temp = list.files(pattern="*.feather") #read names of all .feather files
# #creates names from csv-files in folder;
# names <- setNames(temp, make.names(gsub("*.csv$", "", temp))) #gsub uses regex to replace the specified patterns within a name
#
# #loading df into environment
# list2env(lapply(names,read_feather), envir = .GlobalEnv) #"read_feather" reads in the data from csv
#
# #create a list of all the datatables
# sick_list <- lapply(attr(names,"names"),get)
#
# #combine into a single datatable
# sick_df <- do.call("rbind",sick_list)
#
# remove(list = attr(names,"names"))#removing single df to save RAM
# remove(sick_list)#removing sick_list to save RAM
#
# col_names <- c('userID','longitude','latitude','time','sick','state')
# colnames(sick_df) <- col_names
# setkeyv(sick_df,col_names)
# alarm()
# #loading data from healthy Twitter users
# #setwd("C:/Users/DrosoNeuro/Dropbox/UZH_Master/Masterarbeit/TwitterData/tweets_from_todd/csv_files/one_hundred_csv") # temporarily set WD to folder with files from healthy Twitter users
# setwd("C:/Users/DrosoNeuro/Dropbox/UZH_Master/Masterarbeit/TwitterData/tweets_from_todd/csv_files/one_hundred_csv") # temporarily set WD to folder with files from healthy Twitter users
# temp = list.files(pattern="*.csv") #read names of all .csv files
#
# #creates names from csv-files in folder;
# names <- setNames(temp, make.names(gsub("*.csv$", "", temp))) #gsub uses regex to replace the specified patterns within a name
#
# #loading df into environment
# list2env(lapply(names,fread, header=FALSE), envir = .GlobalEnv)
#
# #create a list of all the datatables
# healthy_list <- lapply(attr(names,"names"),get)
#
# #combine into a single datatable
# healthy_df <- do.call("rbind",healthy_list)
#
# remove(list = attr(names,"names"))#removing single df to save RAM
# remove(healthy_list)#removing sick_list to save RAM
# remove(list= c("names","temp"))
#
# colnames(healthy_df) <- col_names
# setkeyv(healthy_df, col_names) #sets key to column "userID"
# remove(col_names)
# alarm()
setwd(root_path) # set WD back
#save.image(file="subset_all.RData") #saving loaded datatable to prevent loading it from the excel-files the next time
save.image(file="one_hundred.RData")
#save.image(file="sick_tweets.RData")
#fwrite(healthy_df,"healthy_df.csv") #fwrite needs developmental package of "data.table" for now (as of 2016.09.16)
#fwrite(sick_df,"sick_df.csv") #doesn't work yet!!! and isn't faster than simple export of data with feather!
#write_feather(sick_df, "sick_df.feather") #faster than save.image, bute uses more disk space (but shouldn't be used for long-term storage)
#write_feather(healthy_df,"healthy_df.feather") #faster than save.image, but uses more disk space
}) #end of ifelse statement
warnings()
help(cumsum)
cumsum(seq(1:10))
10*log(10)
cumsum(seq(1:1000))
1000*log(1000)
2**log(10)
2**log2(10)
-9/-1
log2(10)
log2(8)
7
1+2+3
8+4+2+1
1+2+3+4
1+2+4+8
1-2^4
7-1
1-2^4/-1
(1-2^4)/-1
