p_eva2
m_binlog <- function(t1,t2,c1,c2,inc,univ)
{
vc <- 1-3/100*t1-6/100*c1+0.5*inc
vt <- -2/100*t2-3.75/100*c2+0.5*univ
p <- exp(vc)/(exp(vc)+exp(vt))
return(p)
}
p_eva2 <- m_binlog(22,18,2,2.1,1,1)
p_eva2
p_matth <- m_binlog(120,100,10,15,0,1)
p_michel <- m_binlog(10,50,3,5,1,0)
p_meri <- m_binlog(25,9,7,2.1,0,0)
p_eva
p_matth
p_meri
p_michel
help(time_zone_converter)
rm(list=ls())
library("gridExtra") #for saving png files in a specific order into pdf
library("ggplot2")
library('ggdendro')
# library("TSclust")
library("ggmap") #used to plot maps
library("maps")
library("scales") # for function alpha()
library("compiler")  # to speed up the computations!
library("plyr")
library("hexbin") #for hexoganal binning
library("rgeos") #for creating maps
library("png") #for reading png files
library("grid") #for arranging png files
library("data.table") #for faster creation of crosstables from data set & for faster searches of datatables; brings about a lot of speed-up! https://github.com/Rdatatable/data.table/wiki/Getting-started
library("bit64") #for loading data with fread
library("lubridate") #for handling time and date information; http://stackoverflow.com/questions/10705328/extract-hours-and-seconds-from-posixct-for-plotting-purposes-in-r
#install_github("rundel/timezone") #needs terminal commands: http://stackoverflow.com/questions/33381421/how-to-upgrade-proj4-for-rgdal
#sudo apt-get install libgdal-dev libproj-dev
library("timezone") #for getting timezones from lat/long data
library("feather") #for fast exporting and importing of data: http://blog.revolutionanalytics.com/2016/05/feather-package.html
#other possibility for fast exporting data is fwrite() using the data.table package: http://blog.h2o.ai/2016/04/fast-csv-writing-for-r/
root_path <- "~/Dropbox/UZH_Master/Masterarbeit/TwitterEpi/ExploratoryAnalysis" # defining root_path containing all relevant documents
script_path <- "~/Dropbox/UZH_Master/Masterarbeit/TwitterEpi/Non_R_Code"
###general functions needed for plotting etc. -------------
# Multiple plot function
#http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
###general functions needed for plotting etc. -------------
# Multiple plot function
#http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
library(grid)
#make a list from the ... arguments and plotlist
plots <- c(list(...), plotlist)
numPlots = length(plots)
# If layout is NULL, then use 'cols' to determine layout
if (is.null(layout)) {
# Make the panel
# ncol: Number of columns of plots
# nrow: Number of rows needed, calculated from # of cols
layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
ncol = cols, nrow = ceiling(numPlots/cols))
}
if (numPlots==1) {
print(plots[[1]])
} else {
# Set up the page
grid.newpage()
pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
# Make each plot, in the correct location
for (i in 1:numPlots) {
# Get the i,j matrix positions of the regions that contain this subplot
matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
layout.pos.col = matchidx$col))
}
}
}
# # NB!! the BEST WAY TO CHOOSE THE BIN IS probably ***** "FD" *****:
# # http://stats.stackexchange.com/questions/798/calculating-optimal-number-of-bins-in-a-histogram-for-n-where-n-ranges-from-30
#
# # LOADING and MERGING DATA FRAMES  -----------------
#
#   setwd(root_path) # setting WD
#   #function to make  selection of datatable based on pre_set coordinates
#
#   #loading files from sick patients
#   #see http://stackoverflow.com/questions/11433432/importing-multiple-csv-files-into-r for explanation about reading several csv-files at once
#   setwd("C:/Users/DrosoNeuro/Dropbox/UZH_Master/Masterarbeit/TwitterData/tweets_from_todd/csv_files/sick_csv") # temporarily set WD to folder with files from healthy Twitter users
#
#   temp = list.files(pattern="*.csv") #read names of all .csv files
#
#   #creates names from csv-files in folder;
#   names <- setNames(temp, make.names(gsub("*.csv$", "", temp))) #gsub uses regex to replace the specified patterns within a name
#
#   #loading df into environment
#   list2env(lapply(names,fread, header=FALSE), envir = .GlobalEnv) #"fread" reads in the data from csv
#
#   #create a list of all the datatables
#   sick_list <- lapply(attr(names,"names"),get)
#
#   #combine into a single datatable
#   sick_df <- do.call("rbind",sick_list)
#
#   remove(list = attr(names,"names"))#removing single df to save RAM
#   remove(sick_list)#removing sick_list to save RAM
#
#   col_names <- c('userID','longitude','latitude','time','sick','state')
#   colnames(sick_df) <- col_names
#   setkeyv(sick_df,col_names)
#   alarm()
#
#   #loading data from healthy Twitter users
#   setwd("C:/Users/DrosoNeuro/Dropbox/UZH_Master/Masterarbeit/TwitterData/tweets_from_todd/csv_files/one_hundred_csv") # temporarily set WD to folder with files from healthy Twitter users
#   temp = list.files(pattern="*.csv") #read names of all .csv files
#
#   #creates names from csv-files in folder;
#   names <- setNames(temp, make.names(gsub("*.csv$", "", temp))) #gsub uses regex to replace the specified patterns within a name
#
#   #loading df into environment
#   list2env(lapply(names,fread, header=FALSE), envir = .GlobalEnv)
#
#   #create a list of all the datatables
#   healthy_list <- lapply(attr(names,"names"),get)
#
#   #combine into a single datatable
#   healthy_df <- do.call("rbind",healthy_list)
#
#   remove(list = attr(names,"names"))#removing single df to save RAM
#   remove(healthy_list)#removing sick_list to save RAM
#   remove(list= c("names","temp"))
#
#   colnames(healthy_df) <- col_names
#   setkeyv(healthy_df, col_names) #sets key to column "userID"
#   remove(col_names)
#   alarm()
#
#   setwd(root_path) # set WD back
#
#   save.image(file="Twitter_datatables.RData") #saving loaded datatable to prevent loading it from the excel-files the next time
#   #fwrite(healthy_df,"healthy_df.csv") #fwrite needs developmental package of "data.table" for now (as of 2016.09.16)
#   #fwrite(sick_df,"sick_df.csv") #doesn't work yet!!! and isn't faster than simple export of data with feather!
#   #write_feather(sick_df, "sick_df.feather") #faster than save.image, bute uses more disk space (but shouldn't be used for long-term storage)
#   #write_feather(healthy_df,"healthy_df.feather") #faster than save.image, but uses more disk space
# EXPLORATORY DATA ANALYSIS ------
#if the code above has been executed once, you can uncomment it and start directly from here
setwd(root_path) # set WD back
load(file="Twitter_datatables.RData") #use if you decided to export the whole working space
# sick_df <- read_feather("sick_df.feather") #potential alternative for exporting working space. is considerably faster, but would need some additional tweaking
# sick_df <- sick_df[,userID:=as.integer64(userID)] #transform to integer64 for readability
# healthy_df <- read_feather("healthy_df.feather")})
# healthy_df <- healthy_df[,userID:=as.integer64(userID)] #transform to integer64 for readability
title_plot<-"All tweets from " #generic title plot used in some functions
# to_analyse <- "healthy_df"
# rm(list=setdiff(ls(), to_analyse)) #removes all entries from workspace except for the datatable that shall be analysed
#
#funtion to make selection of datatable based on coordinate (lon_west,lon_est,lat_south,lat_north)
coord_selection  <- function(datatable,coord_selec)
{
selec <- datatable[datatable[,longitude >=coord_selec[1] & longitude <= coord_selec[2] & latitude >= coord_selec[3] & latitude <=coord_selec[4]],]
#selec <- datatable[which(datatable[,"longitude",]>=coord_selec[1] & datatable[,"longitude"] <= coord_selec[2] & datatable[,"latitude"] >= coord_selec[3] & datatable[,"latitude"] <= coord_selec[4]),] #old way to do it with dataframes
}
#function to make selection of datatable based on coordinate (lon_west,lon_est,lat_south,lat_north); also returns index
coord_selection2  <- function(datatable,coord_selec) #
{
selec <- datatable[datatable[,longitude >=coord_selec[1] & longitude <= coord_selec[2] & latitude >= coord_selec[3] & latitude <=coord_selec[4]],]
index <-datatable[,longitude >=coord_selec[1] & longitude <= coord_selec[2] & latitude >= coord_selec[3] & latitude <=coord_selec[4]]
return(list(selec,index))
}
coord_USA <- c(-125,-66,25,50) #select only tweets from mainland USA
sick_df <- coord_selection(sick_df, coord_USA)
healthy_df <- coord_selection(healthy_df,coord_USA)
explore_data <- function(datatable,sickness_state){ #"sickness_state" takes values "sick" or "healthy" and signifies the state that the users represented in the dataste *should* be in
all_users<-unique(datatable[,userID]) #unique returns a vector, data frame or array like x but with duplicate elements/rows removed; in this case = unique return of user_ID
num_users <- length(all_users)
sick_position <- which(datatable[,sick]==1) #gets position of tweets labelled as asick
num_sick_tweets<-sum(datatable[,sick]==1) #returns number of tweets that are labelled as "sick"
#sick_tweets<-datatable[sick_position,5] #returns entries that are labelled as sick
# NB! n sick tweets != n sick users!!!
sick_users<-unique(datatable[sick_position,userID])
num_sick_users <- length(sick_users)
#getting healthy
healthy_position <- which(datatable[,sick]==0)
#healthy_tweets <- unique(datatable[healthy_position,5]) #returns entries that are labelled as healthy
num_healthy_tweets <- sum(datatable[,sick]==0)
healthy_users <- unique(datatable[healthy_position,userID]) #getting healthy users
num_healthy_users <- length(healthy_users)
#check the total number of false labels
if (sickness_state == "sick"){ #checking whether there are any users in a "sick" datatable that have never been sick, i.e. that healthy_users that don't show up in sick_users
false_label <- healthy_users[!(healthy_users %in% sick_users)]
num_false_label <- length(false_label)
}
else if (sickness_state == "healthy"){
false_label <- sick_users
num_false_label <- num_sick_users
}
out <- list(all_users,num_users,sick_position,num_sick_tweets,sick_users,num_sick_users,healthy_position,num_healthy_tweets,healthy_users,num_healthy_users,false_label,num_false_label)
names(out) <- c("all_users","num_users", "sick_position","num_sick_tweets","sick_users","num_sick_users","healthy_position", "num_healthy_tweets", "healthy_users","num_healthy_users","false_label","num_false_label")
return(out)
}
#get preliminary info from datatables
explore_sick <- explore_data(sick_df,"sick")
str(explore_sick)
explore_sick <- list(explore_sick$false_label) #prune list to save memory
names(explore_sick) <- "false_label"
explore_healthy <- explore_data(healthy_df,"healthy")
str(explore_healthy)
explore_healthy <- list(explore_healthy$false_label) #reduce size of list to save memory
names(explore_healthy) <- "false_label"
datatable <- sick_df
explore <- explore_sick
tag <- "sick_df"
setkey(datatable,"userID")
datatable <- datatable[time!=0,] #removing all entries which don't have a system time
to_import <- read_feather("temporary/to_import.feather") #imports processed dataset with timezones back into R
to_import <- data.table(to_import)
datatable[,timezone:=to_import] #add timezones to datatable
zones_before <- datatable[,.N,by=.(timezone)]
#position of NAs in timezone column
pos_NA <- is.na(datatable[,timezone])
#rough assignment of timezones based on longitude for those entries that came back as NA;
#if tweet was sent from position west of San Diego, we assign the time zone for LA;
#if tweet was sent from position east of Talahassee, we assign the time zone for NY;
#San Diego 32.8242404,-117.3753518
#Talahassee 30.4670648,-84.3969416
datatable <- datatable[pos_NA,timezone:=ifelse(longitude<=-117,"America/Los_Angeles",
ifelse(longitude>=-84,"America/New_York",timezone))]
#get table of timezones to see how many NAs are left (usually, the above fix gets rid of over 90% of NA)
zones_after <- datatable[,.N,by=.(timezone)]
#removing all remaining NAs
datatable <- datatable[!is.na(timezone),]
datatable[,time1:=as.POSIXct(datatable[,time],origin="1970-01-01",tz="UTC")] #transforming time from system time to calendar time UTC
datatable
user332 <- datatable[userID==322,]
user322
user332 <- datatable[userID==322,]
user322 <- datatable[userID==322,]
user322
user322[,time2:=date_converter2(c(user322[1,time],user322[1,timezone]))]
date_converter2 <- function(time_and_zone){
as.character(as.POSIXct(as.numeric(time_and_zone[1]),origin="1970-01-01",tz=time_and_zone[2]),format="%Y-%m-%d %H:%M:%S %Z")
}
user322[,time2:=date_converter2(c(user322[1,time],user322[1,timezone]))]
user322
time_and_zone <- copy(user322[,.(time, timezon)]
time_and_zone <- copy(user322[,.(time, timezone)])
time_and_zone <- copy(user322[,.(time, timezone)])
time_and_zone
user322[,time2:=apply(time_and_zone,1,date_converter2)]
user322
time_and_zone_full <- copy(datatable[,.(time,timezone)])
time_and_zone_full <- copy(datatable[,.(time,timezone)])
datatable[,time2:=apply(time_and_zone_full,1,date_converter2)]
library("profvis")
user322[,time2:=apply(time_and_zone,1,date_converter2)]
})
profvis({
user322[,time2:=apply(time_and_zone,1,date_converter2)]
})
profvis({
user322[,time2:=apply(time_and_zone,1,date_converter2)]})
profvis({
apply(time_and_zone,1,date_converter2)})
help(head)
user322 <- head(datatable,100000)
str(user322)
profvis({
apply(time_and_zone,1,date_converter2)})
user322 <- head(datatable,1000000)
profvis({
apply(time_and_zone,1,date_converter2)})
length(322)
322
length(user322)
user322
time_and_zone322 <- copy(user322[,.(time, timezone)])
profvis({
apply(time_and_zone322,1,date_converter2)})
profvis({
apply(time_and_zone322,1,date_converter2)})
user322 <- head(datatable,50)
time_and_zone322 <- copy(user322[,.(time, timezone)])
profvis({
apply(time_and_zone322,1,date_converter2)}
date_converter2(c(user322[1,time],user322[1,timezone]))
)
profvis({
apply(time_and_zone322,1,date_converter2)}
date_converter2(c(user322[1,time],user322[1,timezone]))
})
profvis({
apply(time_and_zone322,1,date_converter2)
date_converter2(c(user322[1,time],user322[1,timezone]))
})
user322 <- head(datatable,100)
time_and_zone322 <- copy(user322[,.(time, timezone)])
profvis({
apply(time_and_zone322,1,date_converter2)
date_converter2(c(user322[1,time],user322[1,timezone]))
})
time_and_zone322
user322 <- head(datatable,1000)
time_and_zone322 <- copy(user322[,.(time, timezone)])
profvis({
apply(time_and_zone322,1,date_converter2)
date_converter2(c(user322[1,time],user322[1,timezone]))
})
user322 <- head(datatable,10000)
time_and_zone322 <- copy(user322[,.(time, timezone)])
profvis({
apply(time_and_zone322,1,date_converter2)
date_converter2(c(user322[1,time],user322[1,timezone]))
})
user322 <- head(datatable,100000)
time_and_zone322 <- copy(user322[,.(time, timezone)])
profvis({
apply(time_and_zone322,1,date_converter2)
date_converter2(c(user322[1,time],user322[1,timezone]))
})
profvis({
date_converter2(c(user322[1,time],user322[1,timezone]))
})
help("profvis")
profvis({
date_converter2(c(user322[1,time],user322[1,timezone]))
},interval = 0.005)
user322 <- head(datatable,100)
time_and_zone322 <- copy(user322[,.(time, timezone)])
profvis({
apply(time_and_zone322,1,date_converter2)
date_converter2(c(user322[1,time],user322[1,timezone]))
},interval = 0.005)
help(with)
with(time_and_zone322,date_converter2)
library(compiler)
date_converter2_comp <- cmpfun(date_converter2)
user322 <- head(datatable,10000)
time_and_zone322 <- copy(user322[,.(time, timezone)])
profvis({
apply(time_and_zone322,1,date_converter2)
apply(time_and_zone322,1,date_converter2_comp)
})
library(rbenchmark)
install.packages("rbenchmark")
library(rbenchmark)
benchmark( apply(time_and_zone322,1,date_converter2)
apply(time_and_zone322,1,date_converter2_comp)
)
benchmark( apply(time_and_zone322,1,date_converter2),
apply(time_and_zone322,1,date_converter2_comp)
)
user322 <- head(datatable,100)
time_and_zone322 <- copy(user322[,.(time, timezone)])
benchmark( apply(time_and_zone322,1,date_converter2),
apply(time_and_zone322,1,date_converter2_comp)
,replications=50)
datatable[,.(time,timezone),with=F]
datatable[,.(time,time_zone),with=F]
datatable
datatable[,.(time,time_zone),with=T]
datatable[,.(time,timezone),with=T]
time_and_zone[,,with=F]
time_and_zone[,cnames,with=F]
time_and_zone[,c("time","timezone"),with=F]
str(time_and_zone[,c("time","timezone"),with=F])
help(lapply)
time_and_zone[,time2:=apply(time_and_zone,1,date_converter2)]
benchmark( apply(time_and_zone322,1,date_converter2),
apply(time_and_zone322,1,date_converter2_comp),
time_and_zone[,time2:=apply(time_and_zone,1,date_converter2)]
,replications=50)
user322 <- head(datatable,10000)
time_and_zone322 <- copy(user322[,.(time, timezone)])
user322 <- head(datatable,100)
time_and_zone322 <- copy(user322[,.(time, timezone)])
benchmark( apply(time_and_zone322,1,date_converter2),
apply(time_and_zone322,1,date_converter2_comp),
time_and_zone322[,time2:=apply(time_and_zone,1,date_converter2)]
,replications=50)
warnings()
benchmark( apply(time_and_zone322,1,date_converter2),
apply(time_and_zone322,1,date_converter2_comp),
time_and_zone322[,time2:=apply(time_and_zone322,1,date_converter2)]
,replications=50)
help("as.POSIXct")
zones <- datatable[,.N,by=.(timezone)]
zones
str(zones)
zones$timezone
for (x in zones)
{
print(x)
}
zones$timezone
for (x in zones$timezone)
{
print(x)
}
date_converter2 <- function(datatable){
zones <- datatable[,.N,by=.(timezone)]
for (x in zones$timezone)
{
datatable[timezone==x,time2:=lapply(datatable[timezone==x,time],as.POSIXct,origin"1970-01-01",tz=x)]
}
#as.character(as.POSIXct(as.numeric(time_and_zone[1]),origin="1970-01-01",tz=time_and_zone[2]),format="%Y-%m-%d %H:%M:%S %Z")
}
date_converter2 <- function(datatable){
zones <- datatable[,.N,by=.(timezone)]
for (x in zones$timezone)
{
datatable[timezone==x,time2:=lapply(datatable[timezone==x,time],as.POSIXct,origin="1970-01-01",tz=x)]
}
#as.character(as.POSIXct(as.numeric(time_and_zone[1]),origin="1970-01-01",tz=time_and_zone[2]),format="%Y-%m-%d %H:%M:%S %Z")
}
date_converter2(user322)
user322
user322
date_converter2(datatable)
date_converter2A <- function(datatable){
as.character(as.POSIXct(as.numeric(time_and_zone[1]),origin="1970-01-01",tz=time_and_zone[2]),format="%Y-%m-%d %H:%M:%S %Z")
}
date_converter2B <- function(datatable){
zones <- datatable[,.N,by=.(timezone)]
for (x in zones$timezone)
{
datatable[timezone==x,time2:=lapply(datatable[timezone==x,time],as.POSIXct,origin="1970-01-01",tz=x)]
}
#as.character(as.POSIXct(as.numeric(time_and_zone[1]),origin="1970-01-01",tz=time_and_zone[2]),format="%Y-%m-%d %H:%M:%S %Z")
}
benchmark(date_converter2A(user322),date_converter2B(user322),replications = 50)
time_and_zone322 <- copy(user322[,.(time, timezone)])
benchmark(date_converter2A(time_and_zone),date_converter2B(user322),replications = 50)
date_converter2A <- function(time_and_zone){
as.character(as.POSIXct(as.numeric(time_and_zone[1]),origin="1970-01-01",tz=time_and_zone[2]),format="%Y-%m-%d %H:%M:%S %Z")
}
date_converter2A(time_and_zone322)
time_and_zone322
date_converter2A(time_and_zone)
time_and_zone[@]
time_and_zone[2]
time_and_zone
benchmark(apply(time_and_zone,1,date_converter2A),date_converter2B(user322),replications = 50)
user322 <- head(datatable,1000)
time_and_zone322 <- copy(user322[,.(time, timezone)])
benchmark(apply(time_and_zone322,1,date_converter2A),date_converter2B(user322),replications = 50)
warnings
warnings()
user322
user322
profvis({
date_converter2B(user322)
apply(time_and_zone322,1,date_converter2A)
})
date_converter2B(user322)
user322
str(user322[,time2])
user322
user322 <- head(datatable,1000)
time_and_zone322 <- copy(user322[,.(time, timezone)])
profvis({
date_converter2B(user322)
apply(time_and_zone322,1,date_converter2A)
})
user322 <- head(datatable,1000)
user322
date_converter2B(user322)
str(user322)
date_converter2B <- function(datatable){
zones <- datatable[,.N,by=.(timezone)]
for (x in zones$timezone)
{
#datatable[timezone==x,time2:=as.character(lapply(datatable[timezone==x,time],as.POSIXct,origin="1970-01-01",tz=x))]
datatable[timezone==x,time2:=as.POSIXct(datatable[timezone==x,time],origin="1970-01-01",tz=x)]
}
#as.character(as.POSIXct(as.numeric(time_and_zone[1]),origin="1970-01-01",tz=time_and_zone[2]),format="%Y-%m-%d %H:%M:%S %Z")
}
profvis({
date_converter2B(user322)
apply(time_and_zone322,1,date_converter2A)
})
user322
user322
user322 <- head(datatable,1000)
date_converter2B(user322)
user322
user322
user322 <- head(datatable,1000)
time_and_zone322 <- copy(user322[,.(time, timezone)])
profvis({
date_converter2B(user322)
apply(time_and_zone322,1,date_converter2A)
})
benchmark(date_converter2B(user322),replications=50)
user322 <- head(datatable,100000)
benchmark(date_converter2B(user322),replications=50)
date_converter2B(datatable)
datatable
datatable
